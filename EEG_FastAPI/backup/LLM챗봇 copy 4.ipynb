{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "6f84b9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸° - ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# part 1 (0) :\n",
    "#==========================================================\n",
    "# ğŸ“Œ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "#==========================================================\n",
    "import os, sys, random, json, subprocess\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ì•ˆì „ ì„í¬íŠ¸ (ìˆìœ¼ë©´ ì“°ê³ , ì—†ìœ¼ë©´ None)\n",
    "try:\n",
    "    import torch\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "# LangChain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"âœ… ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸° - ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "8c11286c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì‹œë“œ ê³ ì • - ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# part 1 (1):\n",
    "#==========================================================\n",
    "# ğŸ“Œ ì‹œë“œ ê³ ì •(ì¬í˜„ì„±)\n",
    "#==========================================================\n",
    "SEED = int(os.getenv(\"SEED\", \"2024\"))\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch is not None:\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.manual_seed(SEED)\n",
    "            torch.cuda.manual_seed_all(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        pass\n",
    "print(\"âœ… ì‹œë“œ ê³ ì • - ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "b95b08c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… IS_GOOGLE=False, IS_KAGGLE=False, IS_LOCAL=True\n"
     ]
    }
   ],
   "source": [
    "# part 1 (2) \n",
    "#===============================================================================\n",
    "# â–¶ ì‘ì—…í™˜ê²½ í”Œë˜ê·¸\n",
    "#===============================================================================\n",
    "IS_GOOGLE = True if 'google.colab'           in sys.modules   else False\n",
    "IS_KAGGLE = True if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ    else False\n",
    "IS_LOCAL  = True if not (IS_GOOGLE or IS_KAGGLE)              else False\n",
    "print(f\"âœ… IS_GOOGLE={IS_GOOGLE}, IS_KAGGLE={IS_KAGGLE}, IS_LOCAL={IS_LOCAL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0d841840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI API í‚¤ ì¤€ë¹„ë¨\n",
      "âœ… ìƒìˆ˜ì„¤ì • - ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# part 1 (3) \n",
    "#==========================================================\n",
    "# ğŸ“Œ ìƒìˆ˜ì„¤ì • (ëª¨ë¸/í† í°/ë©”ì‹œì§€) + api_token.txt ë¡œë“œ\n",
    "#==========================================================\n",
    "API_TOKEN_PATH  = os.getenv(\"API_TOKEN\", \"./api_token.txt\")\n",
    "OPENAI_API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "if not OPENAI_API_KEY and os.path.exists(API_TOKEN_PATH):\n",
    "    try:\n",
    "        with open(API_TOKEN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            _k = f.read().strip()\n",
    "        if _k:\n",
    "            os.environ[\"OPENAI_API_KEY\"] = _k\n",
    "            OPENAI_API_KEY = _k\n",
    "            print(\"âœ… OpenAI API í‚¤ë¥¼ api_token.txtì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ API í† í° íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    print(\"âœ… OpenAI API í‚¤ ì¤€ë¹„ë¨\")\n",
    "else:\n",
    "    print(\"âš ï¸ OPENAI_API_KEY ë¯¸ì„¤ì • â€” ì‹¤í–‰ ì „ ë°˜ë“œì‹œ ì„¤ì • ë˜ëŠ” api_token.txt ì¤€ë¹„\")\n",
    "\n",
    "# ğŸ”§ ëª¨ë¸ëª…\n",
    "CHAT_MODEL_NAME  = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4o-mini\")\n",
    "EMBED_MODEL_NAME = os.getenv(\"OPENAI_EMBED_MODEL\", \"text-embedding-3-small\")\n",
    "\n",
    "# ğŸ”¢ í† í°/íŒŒë¼ë¯¸í„°\n",
    "MAX_OUTPUT_TOKENS = int(os.getenv(\"MAX_OUTPUT_TOKENS\", \"800\"))\n",
    "TEMPERATURE       = float(os.getenv(\"TEMPERATURE\", \"0.2\"))\n",
    "\n",
    "# ğŸ›¡ ë„ë©”ì¸ ì œí•œ ë©”ì‹œì§€\n",
    "OFF_TOPIC_MESSAGE = \"ì¹˜ë§¤ê´€ë ¨ ìƒë‹´ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\"\n",
    "\n",
    "# ğŸ§­ ê³ ì • ì§ˆë¬¸ ìƒìˆ˜ (ìš”ì²­ í˜•ì‹)\n",
    "Q1 = 'ìì£¼ ì“°ë˜ ë¬¼ê±´ ì´ë¦„ì´ ê°‘ìê¸° ìƒê°ì•ˆ ë‚œì ì´ ìˆë‚˜ìš”?'\n",
    "Q2 = 'ëŒ€í™”ì¤‘ë‹¨ì–´ê°€ ì˜ ë– ì˜¤ë¥´ì§€ ì•Šì•„ì„œ ê³¤ë€í–ˆë˜ ì ì´ ìˆë‚˜ìš”?'\n",
    "Q3 = 'ê°€ì¡±ì´ë‚˜ ì§€ì¸ì´ í‰ì†Œì™€ ë‹¤ë¥´ë‹¤ê³  í•œì ì´ ìˆë‚˜ìš”?'\n",
    "Q4 = 'ìµœê·¼ì— ë¶ˆí¸í–ˆë˜ ì ì´ë‚˜ ê±±ì •ë˜ëŠ” ì ì´ ìˆë‚˜ìš”?'\n",
    "\n",
    "GUIDE_QUESTIONS = [Q1, Q2, Q3, Q4]\n",
    "print(\"âœ… ìƒìˆ˜ì„¤ì • - ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "41754079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í˜ë¥´ì†Œë‚˜/í…œí”Œë¦¿ ì—…ë°ì´íŠ¸(ì„¹ì…˜3 ê³ ì •) - ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# part 2 (1) \n",
    "#==========================================================\n",
    "# ğŸ“Œ í˜ë¥´ì†Œë‚˜, í…œí”Œë¦¿, ì •ì±… (ì„¹ì…˜3: ì‚¬ì „ê³ ì • ë¶ˆë¦¿ ì‚¬ìš©)\n",
    "#==========================================================\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "SYSTEM_PERSONA = '''\\\n",
    "ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ìƒë‹´í•˜ëŠ” ì˜í•™(ì¹˜ë§¤ì§„ë‹¨) ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.\n",
    "- ì—­í• : í™˜ì/ë³´í˜¸ìì˜ ì„œìˆ (STT í…ìŠ¤íŠ¸)ì„ ì½ê³ , ì§€ì •ëœ í…œí”Œë¦¿ìœ¼ë¡œ ê°„ê²°í•˜ê³  ì²´ê³„ì ì¸ ìš”ì•½ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "- íƒœë„: ì •ì¤‘í•˜ê³  ì „ë¬¸ì ì´ë©°, ê³¼ë„í•œ í™•ì‹ /ì§„ë‹¨ì„ í”¼í•˜ê³  ì•ˆì „ìˆ˜ì¹™ì„ ê°•ì¡°í•©ë‹ˆë‹¤.\n",
    "- ë²”ìœ„: ì¹˜ë§¤, ê²½ë„ì¸ì§€ì¥ì• , ê´€ë ¨ ì¦ìƒ/ê²€ì‚¬/ì¼ìƒ ì•ˆì „/ê°€ì¡± êµìœ¡ê³¼ ê°™ì€ ì£¼ì œì— í•œì •í•©ë‹ˆë‹¤.\n",
    "- ê¸ˆì§€: ì¹˜ë§¤ìƒë‹´ê³¼ ë¬´ê´€í•œ ì •ë³´(ì˜ˆ: íŒ¨ìŠ¤íŠ¸í‘¸ë“œ ì‹ ë©”ë‰´, ìŒë£Œ ë‹¹ë¶„ ë“±)ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± ê¸ˆì§€.\n",
    "- ì¶œë ¥: ë°˜ë“œì‹œ ì•„ë˜ 'ìš”ì•½ í…œí”Œë¦¿'ì˜ <ìš”ì•½> ì„¹ì…˜ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤. (<ì§ˆë¬¸>, <STT>ëŠ” ì¶œë ¥ ê¸ˆì§€)\n",
    "'''\n",
    "\n",
    "SUMMARY_TEMPLATE = '''\\\n",
    "<ìš”ì•½>\n",
    "1. **ì£¼ ì¦ìƒ**\n",
    "{symptoms_bullets}\n",
    "\n",
    "2. **ìƒë‹´ë‚´ìš©**\n",
    "{counselling_bullets}\n",
    "\n",
    "3. **ì‹¬ë¦¬ìƒíƒœ**\n",
    "{psych_bullets}\n",
    "\n",
    "4. **AI í•´ì„**\n",
    "{ai_interp_bullets}\n",
    "\n",
    "5. **ì£¼ì˜ì‚¬í•­**\n",
    "{caution_bullets}\n",
    "'''\n",
    "\n",
    "SUMMARISE_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "      (\"system\", SYSTEM_PERSONA + \"\\n\\n\"\n",
    "        \"ì•„ë˜ 'ì°¸ê³  ë¬¸ì„œ(ìš”ì•½)'ëŠ” ìµœì‹  ì§€ì‹ ë³´ê°•ì„ ìœ„í•œ ì°¸ê³ ìš©ì…ë‹ˆë‹¤. \"\n",
    "        \"ê·¼ê±°ê°€ ë¶ˆëª…í™•í•˜ë©´ ê³¼ë„í•œ ë‹¨ì • ëŒ€ì‹  ì¡°ì‹¬ìŠ¤ëŸ¬ìš´ í•´ì„ì„ ì œì‹œí•˜ì„¸ìš”.\\n\"\n",
    "        \"ë°˜ë“œì‹œ 'ìš”ì•½ í…œí”Œë¦¿' êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©° <ìš”ì•½> ì„¹ì…˜ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.\"),\n",
    "      (\"human\",\n",
    "       \"ê°€ì´ë“œ ì§ˆë¬¸(ê³ ì • 4ê°œ ì¤‘ ì„ íƒ): {guide_question}\\n\\n\"\n",
    "       \"ì°¸ê³  ë¬¸ì„œ(ìš”ì•½):\\n{rag_context}\\n\\n\"\n",
    "       \"ì‚¬ìš©ì STT ì›ë¬¸:\\n{transcript}\\n\\n\"\n",
    "       \"ìš”êµ¬ì‚¬í•­:\\n\"\n",
    "       \"1) 'ì£¼ ì¦ìƒ'ì—ëŠ” í•µì‹¬ ì¦ìƒë§Œ '-' ë¶ˆë¦¿ìœ¼ë¡œ ê°„ê²°íˆ.\\n\"\n",
    "       \"2) 'ìƒë‹´ë‚´ìš©'ì—ëŠ” êµ¬ì²´ì  ì‚¬ê±´/ì§„ìˆ  ì¤‘ì‹¬ìœ¼ë¡œ ë¶ˆë¦¿í™”.\\n\"\n",
    "       \"3) 'ì‹¬ë¦¬ìƒíƒœ'ëŠ” **ì•„ë˜ ì œê³µëœ ë¶ˆë¦¿ì„ 'ê·¸ëŒ€ë¡œ' ì‚¬ìš©**í•©ë‹ˆë‹¤(ë¬¸êµ¬/ìˆœì„œ ì„ì˜ ë³€ê²½ ê¸ˆì§€). ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤ìŠ¤ë¡œ ì‘ì„±.\\n\"\n",
    "       \"   ì œê³µ ë¶ˆë¦¿:\\n{psych_bullets_fixed}\\n\"\n",
    "       \"4) 'AI í•´ì„'ì€ ë³‘ëª… ë‹¨ì • ê¸ˆì§€(ì˜ˆ: '~ê°€ëŠ¥ì„± ì‹œì‚¬').\\n\"\n",
    "       \"5) 'ì£¼ì˜ì‚¬í•­'ì€ ì¼ìƒ ì•ˆì „, ì •ì„œì  ì§€ì§€, ê²€ì§„ ê¶Œê³  ë“±ì„ í¬í•¨.\\n\"\n",
    "       \"6) ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥(ì˜¤ì§ <ìš”ì•½> ì„¹ì…˜ë§Œ):\\n\"\n",
    "       \"{summary_template}\\n\"\n",
    "       \"ì¶œë ¥ ì‹œ í•œêµ­ì–´ ë”°ì˜´í‘œ(â€œ)ë¥¼ ìœ ì§€í•˜ì„¸ìš”.\"\n",
    "      )\n",
    "    ]\n",
    ")\n",
    "print(\"âœ… í˜ë¥´ì†Œë‚˜/í…œí”Œë¦¿ ì—…ë°ì´íŠ¸(ì„¹ì…˜3 ê³ ì •) - ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "d6eae685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM/ì„ë² ë”©/ë©€í‹°ê²€ìƒ‰(+ddgs, ì¬ë­í¬) - ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# part 2 (2) \n",
    "#==========================================================\n",
    "# ğŸ“Œ LLM/ì„ë² ë”©/ì›¹ê²€ìƒ‰(RAG) êµ¬ì„± + Multi-Search + Rerank-Then-Summarise\n",
    "#     - DuckDuckGoëŠ” ddgs ì§ì ‘ í˜¸ì¶œë¡œ ê²½ê³  ì œê±°\n",
    "#==========================================================\n",
    "import os, sys, subprocess, json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=CHAT_MODEL_NAME,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_OUTPUT_TOKENS,\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "embeddings = OpenAIEmbeddings(model=EMBED_MODEL_NAME, api_key=OPENAI_API_KEY)\n",
    "\n",
    "# --- ê³µìš© ìœ í‹¸ ---\n",
    "def _try_import(module_path: str, cls_name: str):\n",
    "    try:\n",
    "        mod = __import__(module_path, fromlist=[cls_name])\n",
    "        return getattr(mod, cls_name)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _norm_list_result(engine: str, item: Dict[str, Any]) -> Document:\n",
    "    title   = item.get(\"title\") or item.get(\"name\") or \"\"\n",
    "    link    = item.get(\"link\") or item.get(\"url\") or item.get(\"href\") or item.get(\"source\") or \"\"\n",
    "    snippet = item.get(\"snippet\") or item.get(\"body\") or item.get(\"content\") or item.get(\"description\") or \"\"\n",
    "    text = f\"{title}\\n{snippet}\\nURL: {link}\"\n",
    "    return Document(page_content=text, metadata={\"source\": link, \"title\": title, \"engine\": engine})\n",
    "\n",
    "def _result_to_docs(engine_name: str, result: Any) -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    if isinstance(result, list):\n",
    "        for it in result:\n",
    "            if isinstance(it, dict):\n",
    "                docs.append(_norm_list_result(engine_name, it))\n",
    "            else:\n",
    "                docs.append(Document(page_content=str(it), metadata={\"engine\": engine_name}))\n",
    "    else:\n",
    "        docs.append(Document(page_content=str(result), metadata={\"engine\": engine_name}))\n",
    "    return docs\n",
    "\n",
    "# --- ddgs (DuckDuckGo native) ---\n",
    "def _ensure_ddgs():\n",
    "    try:\n",
    "        from ddgs import DDGS\n",
    "        return DDGS\n",
    "    except Exception:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"ddgs\"], stdout=subprocess.DEVNULL)\n",
    "            from ddgs import DDGS\n",
    "            return DDGS\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "DDGS = _ensure_ddgs()\n",
    "\n",
    "def ddgs_search(query: str, k: int = 5) -> List[Document]:\n",
    "    if DDGS is None:\n",
    "        return []\n",
    "    docs: List[Document] = []\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            # ì°¸ê³ : ddgs.text(...)ëŠ” dict ìŠ¤íŠ¸ë¦¼ì„ ë°˜í™˜ (title, href, body ë“±)\n",
    "            for i, item in enumerate(ddgs.text(query, max_results=k)):\n",
    "                title = item.get(\"title\") or \"\"\n",
    "                link  = item.get(\"href\") or \"\"\n",
    "                body  = item.get(\"body\") or \"\"\n",
    "                docs.append(Document(page_content=f\"{title}\\n{body}\\nURL: {link}\",\n",
    "                                     metadata={\"source\": link, \"title\": title, \"engine\": \"ddgs\"}))\n",
    "                if i+1 >= k:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ddgs ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "    return docs\n",
    "\n",
    "# --- Google CSE / SerpAPI / Bing (ìˆì„ ë•Œë§Œ ì‚¬ìš©) ---\n",
    "GoogleSearchAPIWrapper = _try_import(\"langchain_community.utilities\", \"GoogleSearchAPIWrapper\")\n",
    "SerpAPIWrapper         = _try_import(\"langchain_community.utilities\", \"SerpAPIWrapper\")\n",
    "BingSearchAPIWrapper   = _try_import(\"langchain_community.utilities\", \"BingSearchAPIWrapper\")\n",
    "WikipediaLoader        = _try_import(\"langchain_community.document_loaders\", \"WikipediaLoader\")\n",
    "Chroma                 = _try_import(\"langchain_community.vectorstores\", \"Chroma\")\n",
    "\n",
    "def _safe_init_google():\n",
    "    if GoogleSearchAPIWrapper and os.getenv(\"GOOGLE_API_KEY\") and os.getenv(\"GOOGLE_CSE_ID\"):\n",
    "        try:\n",
    "            return GoogleSearchAPIWrapper()\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def _safe_init_serpapi():\n",
    "    if SerpAPIWrapper and os.getenv(\"SERPAPI_API_KEY\"):\n",
    "        try:\n",
    "            return SerpAPIWrapper()\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def _safe_init_bing():\n",
    "    if BingSearchAPIWrapper and os.getenv(\"BING_SUBSCRIPTION_KEY\"):\n",
    "        try:\n",
    "            return BingSearchAPIWrapper()\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "gse  = _safe_init_google()\n",
    "serp = _safe_init_serpapi()\n",
    "bing = _safe_init_bing()\n",
    "\n",
    "def multi_engine_search(query: str, k: int = 5) -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    # ìš°ì„ ìˆœìœ„: Google CSE â†’ Bing â†’ SerpAPI â†’ ddgs\n",
    "    if gse:\n",
    "        try:\n",
    "            res = gse.results(query, k)\n",
    "            docs.extend(_result_to_docs(\"google\", res))\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ google ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "    if bing:\n",
    "        try:\n",
    "            res = bing.results(query, k)\n",
    "            docs.extend(_result_to_docs(\"bing\", res))\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ bing ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "    if serp:\n",
    "        try:\n",
    "            res = serp.results(query, k)\n",
    "            docs.extend(_result_to_docs(\"serpapi\", res))\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ serpapi ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "    # ddgsëŠ” í•­ìƒ ë§ˆì§€ë§‰ í´ë°±ìœ¼ë¡œ ì‹¤í–‰\n",
    "    docs.extend(ddgs_search(query, k))\n",
    "\n",
    "    # ê°„ë‹¨ ì¤‘ë³µ ì œê±°\n",
    "    seen = set()\n",
    "    uniq_docs: List[Document] = []\n",
    "    for d in docs:\n",
    "        key = (d.metadata.get(\"source\") or \"\") + \"|\" + (d.metadata.get(\"title\") or \"\") + \"|\" + d.page_content[:80]\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        uniq_docs.append(d)\n",
    "    return uniq_docs\n",
    "\n",
    "def _embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    return embeddings.embed_documents(texts)\n",
    "\n",
    "def _embed_query(text: str) -> List[float]:\n",
    "    return embeddings.embed_query(text)\n",
    "\n",
    "def _cosine(a: List[float], b: List[float]) -> float:\n",
    "    a = np.array(a, dtype=np.float32); b = np.array(b, dtype=np.float32)\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + 1e-12\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def build_rag_context_rerank(\n",
    "    transcript: str,\n",
    "    queries: List[str],\n",
    "    k_search: int = 8,\n",
    "    k_vec: int    = 12,\n",
    "    k_rerank: int = 8,\n",
    "    k_final: int  = 4\n",
    ") -> str:\n",
    "    # 1) ë©€í‹° ì—”ì§„ ê²€ìƒ‰ ì§‘ê³„\n",
    "    all_docs: List[Document] = []\n",
    "    for q in queries:\n",
    "        all_docs.extend(multi_engine_search(q, k=max(3, k_search // max(1, len(queries)) + 1)))\n",
    "\n",
    "    # 2) ìœ„í‚¤ ë³´ê°•\n",
    "    if (WikipediaLoader is not None) and (len(all_docs) < 2):\n",
    "        try:\n",
    "            wiki_docs = WikipediaLoader(query=\"ì¹˜ë§¤\", load_max_docs=3).load()\n",
    "            all_docs.extend(wiki_docs)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Wikipedia ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    if not all_docs:\n",
    "        return \"\"\n",
    "\n",
    "    # 3) 1ì°¨ í›„ë³´ (Chroma ì‚¬ìš© ê°€ëŠ¥ ì‹œ)\n",
    "    if Chroma is not None:\n",
    "        try:\n",
    "            vs = Chroma.from_documents(all_docs, embeddings)\n",
    "            retriever = vs.as_retriever(search_kwargs={\"k\": min(k_vec, max(1, len(all_docs)))})\n",
    "            seed_query = \" \".join(queries[:3]) or \"ì¹˜ë§¤ ê²½ë„ì¸ì§€ì¥ì•  ì´ˆê¸° ì¦ìƒ ì•ˆì „ ê°€ì¡± êµìœ¡ í‰ê°€ë„êµ¬\"\n",
    "            vec_docs = retriever.invoke(seed_query)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ë²¡í„°ìŠ¤í† ì–´/ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "            vec_docs = all_docs[:k_vec]\n",
    "    else:\n",
    "        vec_docs = all_docs[:k_vec]\n",
    "\n",
    "    # 4) ì¬ë­í¬ (ì¿¼ë¦¬+ì›ë¬¸ ê¸°ë°˜ ì½”ì‚¬ì¸)\n",
    "    combined_query = ((\" \".join(queries)) + \" \" + transcript[:600]).strip()\n",
    "    q_emb = _embed_query(combined_query)\n",
    "    cand_docs  = vec_docs[:k_rerank]\n",
    "    cand_texts = [d.page_content for d in cand_docs]\n",
    "    cand_embs  = _embed_texts(cand_texts)\n",
    "    scores = [(_cosine(q_emb, e), i) for i, e in enumerate(cand_embs)]\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    top_docs = [cand_docs[i] for (s, i) in scores[:k_final]]\n",
    "    rag_text = \"\\n\\n\".join([d.page_content[:1200] for d in top_docs])\n",
    "    return rag_text\n",
    "\n",
    "print(\"âœ… LLM/ì„ë² ë”©/ë©€í‹°ê²€ìƒ‰(+ddgs, ì¬ë­í¬) - ì¤€ë¹„ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e229395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê°ì§€ê¸°(ì™„í™” íŠœë‹) - ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# part 2 (3)\n",
    "#==========================================================\n",
    "# ğŸ“Œ ëª¨ë¸ ê¸°ë°˜ ì˜¨í† í”½ ê°ì§€ê¸° (ë¬´í•˜ë“œì½”ë”©)\n",
    "#   - Zero-shot LLM ë¶„ë¥˜(ì ìˆ˜+ê·¼ê±°ìŠ¤íŒ¬)\n",
    "#   - ì„¸ì…˜ prior(ê°€ì´ë“œ ì§ˆë¬¸ ì´í›„ ê°€ì‚°)ë§Œ ì‚¬ìš©\n",
    "#==========================================================\n",
    "import json, time\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "CLASSIFY_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"ë„ˆëŠ” ì…ë ¥ì´ â€˜ì¹˜ë§¤/ì¸ì§€ì¥ì•  ìƒë‹´â€™ì¸ì§€ íŒë‹¨í•˜ëŠ” ë¶„ë¥˜ê¸°ë‹¤. \"\n",
    "     \"íŒë‹¨ ê¸°ì¤€: (a) ê¸°ì–µë ¥ ì €í•˜/ë‹¨ì–´ íšŒìƒ ê³¤ë€/ì–¸ì–´ ìœ ì°½ì„± ì €í•˜/ë°©í–¥ê°ê° ë¬¸ì œ/ì¼ìƒìƒí™œ ê³¤ë€/ì •ì„œë°˜ì‘ ë“±ì˜ êµ¬ì²´ì  ì„œìˆ , \"\n",
    "     \"(b) ë³´í˜¸ì ê´€ì°°/ê²€ì‚¬/í‰ê°€/ì•ˆì „ ë¬¸ì œ/ê°€ì¡± êµìœ¡. \"\n",
    "     \"ë©”íƒ€ ëŒ€í™”(ë„ˆëŠ” ëˆ„êµ¬ëƒ, ìƒë‹´ ë˜ëƒ ë“±), ì¼ë°˜ ì¡ë‹´, ë¹„ì¹˜ë§¤ ì¼ë°˜ ê±´ê°•/ì¼ìƒì€ off_topic. \"\n",
    "     \"ì˜¤ì§ JSONë§Œ ì¶œë ¥: \"\n",
    "     \"{\\\"on_topic\\\": true|false, \\\"score\\\": 0.0~1.0, \"\n",
    "     \"\\\"evidence_spans\\\": [\\\"...\\\", \\\"...\\\"], \\\"reason\\\": \\\"...\\\"}. \"\n",
    "     \"evidence_spansëŠ” ì›ë¬¸ ë°œì·Œ 1~3ê°œ(ê° 6~40ì).\"),\n",
    "    (\"human\", \"ì…ë ¥:\\n\\\"\\\"\\\"\\n{user_text}\\n\\\"\\\"\\\"\\nì˜¤ì§ JSON:\")\n",
    "])\n",
    "\n",
    "judge_llm = ChatOpenAI(model=CHAT_MODEL_NAME, temperature=0.0, max_tokens=220, api_key=OPENAI_API_KEY)\n",
    "judge_chain = CLASSIFY_PROMPT | judge_llm | StrOutputParser()\n",
    "\n",
    "@dataclass\n",
    "class DetectResult:\n",
    "    label: str\n",
    "    on_topic_prob: float\n",
    "    stage_scores: Dict[str,float]\n",
    "    evidence: List[str]\n",
    "\n",
    "DEFAULT_PRIOR = 0.60\n",
    "SESSION_STATE: Dict[str, Dict] = {}\n",
    "\n",
    "def get_session(session_id: Optional[str]) -> Dict:\n",
    "    sid = session_id or \"default\"\n",
    "    if sid not in SESSION_STATE:\n",
    "        SESSION_STATE[sid] = {\"prior\": DEFAULT_PRIOR, \"last_ts\": time.time(), \"history\": []}\n",
    "    return SESSION_STATE[sid]\n",
    "\n",
    "def mark_guide_question_shown(session_id: Optional[str]):\n",
    "    s = get_session(session_id)\n",
    "    s[\"prior\"] = max(s[\"prior\"], 0.75)  # ê°€ì´ë“œ ì§í›„ ì‘ë‹µì€ on-topic priorâ†‘\n",
    "\n",
    "def update_session(session_id: Optional[str], label: str, prob: float):\n",
    "    s = get_session(session_id)\n",
    "    s[\"history\"].append({\"label\": label, \"prob\": prob})\n",
    "    s[\"last_ts\"] = time.time()\n",
    "    last = s[\"history\"][-3:]\n",
    "    s[\"prior\"] = 0.5 * s[\"prior\"] + 0.5 * (sum(h[\"prob\"] for h in last) / len(last))\n",
    "\n",
    "TAU_ON = 0.60           # ìµœì¢… on-topic ì„ê³„\n",
    "BAND   = (0.48, 0.60)   # íšŒìƒ‰ì§€ëŒ€ â†’ ì•ˆë‚´(Abstain)\n",
    "\n",
    "def detect_topic(text: str, session_id: Optional[str]=None) -> DetectResult:\n",
    "    t = (text or \"\").strip()\n",
    "    if not t:\n",
    "        return DetectResult(\"off_topic\", 0.0, {\"llm\": 0.0}, [])\n",
    "\n",
    "    prior = get_session(session_id)[\"prior\"]\n",
    "    try:\n",
    "        raw = judge_chain.invoke({\"user_text\": t})\n",
    "        data = json.loads(raw)\n",
    "        score = float(data.get(\"score\", 0.5))\n",
    "        evid  = [e for e in (data.get(\"evidence_spans\") or []) if isinstance(e, str)]\n",
    "        combined = max(0.0, min(1.0, 0.65 * score + 0.35 * prior))\n",
    "    except Exception:\n",
    "        # LLM ì‹¤íŒ¨ ì‹œ priorë§Œìœ¼ë¡œ íŒì •(ê°€ì´ë“œ ì§í›„ ëˆ„ë½ ë°©ì§€)\n",
    "        evid = []\n",
    "        combined = prior\n",
    "\n",
    "    if combined >= TAU_ON:\n",
    "        label = \"on_topic\"\n",
    "    elif BAND[0] <= combined < BAND[1]:\n",
    "        label = \"abstain\"\n",
    "    else:\n",
    "        label = \"off_topic\"\n",
    "\n",
    "    return DetectResult(label, combined, {\"llm\": combined}, evid)\n",
    "\n",
    "print(\"âœ… ì˜¨í† í”½ ê°ì§€ê¸°(ëª¨ë¸ ê¸°ë°˜) - ì¤€ë¹„ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "cce58fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê²€ìƒ‰ì§ˆì˜ ìƒì„±ê¸° - ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# part 3 (1) \n",
    "#==========================================================\n",
    "# ğŸ“Œ ê²€ìƒ‰ì§ˆì˜ ìƒì„±ê¸° (STT â†’ ê´€ë ¨ ê²€ìƒ‰ì–´ 2~4ê°œ)\n",
    "#==========================================================\n",
    "QUERY_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "      (\"system\",\n",
    "       \"ë„ˆëŠ” ì˜ë£Œ ìƒë‹´ ë³´ì¡° ê²€ìƒ‰ì–´ ìƒì„±ê¸°ì´ë‹¤. ì…ë ¥ STTì—ì„œ í•µì‹¬ ì¦ìƒ/í‚¤ì›Œë“œë¥¼ ë½‘ì•„ \"\n",
    "       \"ì¹˜ë§¤/ê²½ë„ì¸ì§€ì¥ì• ì™€ ê´€ë ¨ëœ í•œêµ­ì–´ ê²€ìƒ‰ì§ˆì˜ 2~4ê°œë¥¼ JSON ë°°ì—´ë¡œë§Œ ì¶œë ¥í•˜ë¼.\"),\n",
    "      (\"human\", \"{transcript}\")\n",
    "    ]\n",
    ")\n",
    "query_llm   = ChatOpenAI(model=CHAT_MODEL_NAME, temperature=0.1, max_tokens=120, api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "query_chain = QUERY_PROMPT | query_llm | StrOutputParser()\n",
    "\n",
    "def make_search_queries(transcript: str) -> List[str]:\n",
    "    try:\n",
    "        raw = query_chain.invoke({\"transcript\": transcript})\n",
    "        qs = json.loads(raw)\n",
    "        qs = [q for q in qs if isinstance(q, str)]\n",
    "        base = [\"ì¹˜ë§¤ ì´ˆê¸° ì¦ìƒ\", \"ê²½ë„ì¸ì§€ì¥ì•  ì–¸ì–´ ìœ ì°½ì„±\", \"ì¼ìƒ ì•ˆì „ ê°€ì¡± êµìœ¡\", \"ë‹¨ê¸° ê¸°ì–µë ¥ ì €í•˜ ì›ì¸\"]\n",
    "        return (qs + base)[:4]\n",
    "    except Exception:\n",
    "        return [\"ì¹˜ë§¤ ì´ˆê¸° ì¦ìƒ\", \"ê²½ë„ì¸ì§€ì¥ì•  ì–¸ì–´ ìœ ì°½ì„±\", \"ì¼ìƒ ì•ˆì „ ê°€ì¡± êµìœ¡\", \"ë‹¨ê¸° ê¸°ì–µë ¥ ì €í•˜ ì›ì¸\"]\n",
    "\n",
    "print(\"âœ… ê²€ìƒ‰ì§ˆì˜ ìƒì„±ê¸° - ì¤€ë¹„ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1073931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì‹¬ë¦¬ìƒíƒœ ì¶”ì¶œê¸°(í•œêµ­ì–´ LLM ë„ì¶œí˜•) - ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# part 3 (1.5)\n",
    "#==========================================================\n",
    "# ğŸ“Œ ì‹¬ë¦¬ìƒíƒœ ì¶”ì¶œê¸° (ê°ì •-ê·¼ê±° ìŠ¤íŒ¬ JSON â†’ ë¶ˆë¦¿ ë¬¸ìì—´)\n",
    "#==========================================================\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "EMOTION_SET = [\"ë‘ë ¤ì›€\",\"ë¶ˆì•ˆ\",\"ê±±ì •\",\"ìˆ˜ì¹˜ì‹¬\",\"ë‹¹í™©\",\"ë‹µë‹µ\",\"ë¬´ê¸°ë ¥\",\"í˜¼ë€\",\"ìì±…\"]\n",
    "\n",
    "PSYCH_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"ë‹¤ìŒ í•œêµ­ì–´ ìƒë‹´ ë°œí™”ì—ì„œ ê°ì •-ê·¼ê±° ìŒì„ 1~4ê°œ ì¶”ì¶œí•˜ë¼. \"\n",
    "     \"ê°ì •(emotion)ì€ ë‹¤ìŒ ì¤‘ì—ì„œ ê³ ë¥´ë¼: \" + \",\".join(EMOTION_SET) + \". \"\n",
    "     \"ê·¼ê±°(evidence)ëŠ” **ë°˜ë“œì‹œ ì›ë¬¸ì—ì„œ 6~30ì ì§ë°œì·Œ**. \"\n",
    "     \"JSON ë°°ì—´ë§Œ ì¶œë ¥. ì˜ˆ: [{\\\"emotion\\\":\\\"ë‘ë ¤ì›€\\\",\\\"evidence\\\":\\\"í˜¼ì ì™¸ì¶œí•˜ëŠ” ê²ƒë„ ë¬´ì„­ìŠµë‹ˆë‹¤\\\"}, ...] \"\n",
    "     \"ì¦ê±°ê°€ ë¶ˆì¶©ë¶„í•˜ë©´ ë¹ˆ ë°°ì—´[]ì„ ì¶œë ¥í•˜ë¼.\"),\n",
    "    (\"human\", \"{transcript}\")\n",
    "])\n",
    "_psych_llm = ChatOpenAI(model=CHAT_MODEL_NAME, temperature=0.1, max_tokens=200, api_key=OPENAI_API_KEY)\n",
    "_psych_chain = PSYCH_PROMPT | _psych_llm | StrOutputParser()\n",
    "\n",
    "def build_psych_bullets(transcript: str) -> str:\n",
    "    # 1) LLM ì¶”ì¶œ\n",
    "    pairs = []\n",
    "    try:\n",
    "        raw = _psych_chain.invoke({\"transcript\": transcript})\n",
    "        data = json.loads(raw)\n",
    "        if isinstance(data, list):\n",
    "            for it in data:\n",
    "                emo = (it.get(\"emotion\") or \"\").strip()\n",
    "                ev  = (it.get(\"evidence\") or \"\").strip().strip('â€œ\"').strip()\n",
    "                if emo and ev and 6 <= len(ev) <= 40:\n",
    "                    pairs.append((emo, ev))\n",
    "    except Exception:\n",
    "        pairs = []\n",
    "\n",
    "    # 2) íœ´ë¦¬ìŠ¤í‹± ë³´ê°•(ì—†ì„ ë•Œ)\n",
    "    if not pairs:\n",
    "        heuristics = [\n",
    "            (\"ë‘ë ¤ì›€\",  r\"(ë¬´ì„­|ë‘ë µ)\"),\n",
    "            (\"ë¶ˆì•ˆ\",    r\"ë¶ˆì•ˆ\"),\n",
    "            (\"ê±±ì •\",    r\"ê±±ì •\"),\n",
    "            (\"ë‹¹í™©\",    r\"ë‹¹í™©\"),\n",
    "            (\"ìˆ˜ì¹˜ì‹¬\",  r\"(ì°½í”¼|ìˆ˜ì¹˜)\"),\n",
    "            (\"ë‹µë‹µ\",    r\"ë‹µë‹µ\"),\n",
    "        ]\n",
    "        sents = [s.strip() for s in re.split(r\"[.!?\\n]\", transcript) if s.strip()]\n",
    "        for emo, pat in heuristics:\n",
    "            rex = re.compile(pat)\n",
    "            for s in sents:\n",
    "                if rex.search(s) and 6 <= len(s) <= 40:\n",
    "                    pairs.append((emo, s))\n",
    "                    break\n",
    "            if len(pairs) >= 3:\n",
    "                break\n",
    "\n",
    "    # 3) ë¶ˆë¦¿ ë¬¸ìì—´ êµ¬ì„±\n",
    "    if not pairs:\n",
    "        return \"\"  # í”„ë¡¬í”„íŠ¸ê°€ ë¹„ì—ˆìœ¼ë©´ LLMì´ ìƒì„±\n",
    "\n",
    "    lines = []\n",
    "    for emo, ev in pairs[:4]:\n",
    "        lines.append(f\"- {emo}\\n  - ê·¼ê±° : â€œ{ev}â€\")\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c4feb23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 3 (2)\n",
    "#==========================================================\n",
    "# ğŸ“Œ ìš”ì•½ ì²´ì¸ (ì„¹ì…˜3 ê³ ì • ë¶ˆë¦¿ ì „ë‹¬, ë³€ìˆ˜ ëˆ„ë½ ë°©ì§€)\n",
    "#==========================================================\n",
    "summary_chain = SUMMARISE_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "ABSTAIN_MESSAGE = (\n",
    "    \"ì¹˜ë§¤ ê´€ë ¨ ì¦ìƒÂ·ê²½í—˜ ì¤‘ì‹¬ìœ¼ë¡œ ì¡°ê¸ˆë§Œ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì‹œë©´ ìš”ì•½ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\\n\"\n",
    "    \"ì˜ˆ) ìµœê·¼ì— ìŠì–´ë²„ë¦° ì‚¬ë¡€, ë‹¨ì–´ê°€ ë§‰í˜”ë˜ ìƒí™©, ì™¸ì¶œ ì‹œ ë¶ˆí¸í–ˆë˜ ì  ë“±\"\n",
    ")\n",
    "\n",
    "def summarise_transcript_only_summary(\n",
    "    transcript: str,\n",
    "    guide_question_index: int = 3,\n",
    "    session_id: Optional[str] = None,\n",
    ") -> str:\n",
    "    # 1) ì˜¨í† í”½ ê°ì§€\n",
    "    detect = detect_topic(transcript, session_id=session_id)\n",
    "\n",
    "    if detect.label == \"off_topic\":\n",
    "        update_session(session_id, \"off_topic\", detect.on_topic_prob)\n",
    "        return OFF_TOPIC_MESSAGE\n",
    "\n",
    "    if detect.label == \"abstain\":\n",
    "        update_session(session_id, \"abstain\", detect.on_topic_prob)\n",
    "        return ABSTAIN_MESSAGE\n",
    "\n",
    "    update_session(session_id, \"on_topic\", detect.on_topic_prob)\n",
    "\n",
    "    # 2) RAG ì»¨í…ìŠ¤íŠ¸\n",
    "    guide_question = GUIDE_QUESTIONS[max(0, min(3, int(guide_question_index)))]\n",
    "    queries = make_search_queries(transcript)\n",
    "    rag_context = build_rag_context_rerank(transcript, queries)\n",
    "\n",
    "    # 3) ì„¹ì…˜3(ì‹¬ë¦¬ìƒíƒœ) ê³ ì • ë¶ˆë¦¿ ìƒì„± â€” ë°˜ë“œì‹œ ë³€ìˆ˜ë¡œ ì „ë‹¬\n",
    "    try:\n",
    "        psych_bullets_fixed = build_psych_bullets(transcript).strip()\n",
    "    except Exception:\n",
    "        psych_bullets_fixed = \"\"\n",
    "    if not psych_bullets_fixed:\n",
    "        psych_bullets_fixed = \"(ì—†ìŒ)\"\n",
    "\n",
    "    # 4) ì²´ì¸ í˜¸ì¶œ (ëª¨ë“  ë³€ìˆ˜ ì „ë‹¬)\n",
    "    out = summary_chain.invoke({\n",
    "        \"transcript\": transcript.strip(),\n",
    "        \"rag_context\": (rag_context.strip() if rag_context else \"(ë¬¸ë§¥ ì—†ìŒ)\"),\n",
    "        \"guide_question\": guide_question,\n",
    "        \"summary_template\": SUMMARY_TEMPLATE,\n",
    "        \"psych_bullets_fixed\": psych_bullets_fixed,   # âœ… í•„ìˆ˜ ì „ë‹¬\n",
    "    })\n",
    "    return out.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "15a3d6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: ./summary_1_.txt\n",
      "------ ë¯¸ë¦¬ë³´ê¸° ------\n",
      "<ìš”ì•½>\n",
      "1. **ì£¼ ì¦ìƒ**\n",
      "- ë¬¼ê±´ì„ ìƒì–´ë²„ë¦¼\n",
      "- ì™¸ì¶œ ì‹œ ê¸¸ì„ ìƒì„ê¹Œ ë‘ë ¤ì›€\n",
      "- ì „í™” í†µí™” í›„ ê¸°ì–µ ìƒì‹¤\n",
      "- ë‹¨ì–´ê°€ ì˜ ë– ì˜¤ë¥´ì§€ ì•ŠìŒ\n",
      "\n",
      "2. **ìƒë‹´ë‚´ìš©**\n",
      "- ì•ˆê²½ì´ë‚˜ íœ´ëŒ€í°ì„ ë‘ê³  ì°¾ì§€ ëª»í•´ ê°€ì¡±ì—ê²Œ ìì£¼ ë¬¼ì–´ë´„.\n",
      "- í˜¼ì ì™¸ì¶œí•˜ëŠ” ê²ƒì´ ë¬´ì„­ê³  ê¸¸ì„ ìƒì„ê¹Œ ê±±ì •ë¨.\n",
      "- ì „í™” í†µí™”ë¥¼ í•˜ê³  ë‚˜ë©´ ëŒ€í™” ë‚´ìš©ì„ ê¸ˆë°© ìŠì–´ë²„ë¦¼.\n",
      "- ìì‹ ì˜ ìƒíƒœê°€ ë‚˜ë¹ ì ¸ ê°€ì¡±ì—ê²Œ ì§ì´ ë ê¹Œ ê±±ì •í•¨.\n",
      "- ëŒ€í™” ì¤‘ ë‹¨ì–´ê°€ ì˜ ìƒê°ë‚˜ì§€ ì•Šì•„ ì°½í”¼í•¨.\n",
      "\n",
      "3. **ì‹¬ë¦¬ìƒíƒœ**\n",
      "(ì—†ìŒ)\n",
      "\n",
      "4. **AI í•´ì„**\n",
      "í˜„ì¬ì˜ ì¦ìƒì€ ê²½ë„ì¸ì§€ì¥ì• (MCI)ì™€ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë©°, ì¡°ê¸° ì§„ë‹¨ê³¼ ì¹˜ë£Œê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "5. **ì£¼ì˜ì‚¬í•­**\n",
      "- ì¼ìƒì—ì„œ ë¬¼ê±´ì„ ë‘ëŠ” ìë¦¬ë¥¼ ì •í•´ë‘ê³ , ì™¸ì¶œ ì‹œì—ëŠ” ì•ˆì „í•œ ê²½ë¡œë¥¼ ë¯¸ë¦¬ ê³„íší•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
      "- ê°€ì¡±ê³¼ì˜ ì†Œí†µì„ í†µí•´ ì •ì„œì  ì§€ì§€ë¥¼ ë°›ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
      "- ì¦ìƒì´ ì§€ì†ë˜ê±°ë‚˜ ì•…í™”ë  ê²½ìš° ì „ë¬¸ì˜ì™€ ìƒë‹´í•˜ì—¬ ê²€ì§„ì„ ë°›ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# part 4 (demo)\n",
    "#==========================================================\n",
    "# ğŸ“Œ ë°ëª¨ ì‹¤í–‰: './script_1.txt' â†’ 'summary_1_.txt' ë¡œ ì €ì¥\n",
    "#==========================================================\n",
    "INPUT_TXT  = \"./script_1.txt\"\n",
    "OUTPUT_TXT = \"./summary_1_.txt\"\n",
    "QUESTION_INDEX = 3\n",
    "SESSION_ID = \"demo-session-1\"\n",
    "\n",
    "# ê°€ì´ë“œ ì§ˆë¬¸ì„ ë˜ì§„ ê²ƒìœ¼ë¡œ ê°„ì£¼ â†’ ì„¸ì…˜ prior boost\n",
    "mark_guide_question_shown(SESSION_ID)\n",
    "\n",
    "if not os.path.exists(INPUT_TXT):\n",
    "    print(f\"âš ï¸ {INPUT_TXT} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë°ëª¨ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "    demo_stt = (\n",
    "        \"ìš”ì¦˜ ë¬¼ê±´ì„ ì–´ë””ì— ë‘ì—ˆëŠ”ì§€ ìê¾¸ ìŠì–´ë²„ë ¤ì„œ í˜ë“­ë‹ˆë‹¤. \"\n",
    "        \"ì•ˆê²½ì´ë‚˜ íœ´ëŒ€í°ì„ ë‘ê³ ë„ ì°¾ì§€ë¥¼ ëª»í•´ì„œ ê°€ì¡±ë“¤í•œí…Œ ìì£¼ ë¬¼ì–´ë³´ê²Œ ë¼ìš”. \"\n",
    "        \"ë˜ ë°–ì— ë‚˜ê°”ë‹¤ê°€ ê¸¸ì„ ìƒì„ê¹Œ ë´ í˜¼ì ì™¸ì¶œí•˜ëŠ” ê²ƒë„ ë¬´ì„­ìŠµë‹ˆë‹¤. \"\n",
    "        \"ì§‘ì—ì„œëŠ” ì „í™” í†µí™”ë¥¼ í•˜ê³  ë‚˜ë©´ ê¸ˆë°© ë¬´ìŠ¨ ì´ì•¼ê¸°ë¥¼ í–ˆëŠ”ì§€ ê¸°ì–µì´ ì•ˆ ë‚˜ì„œ ë‹µë‹µí•˜ê³ ìš”. \"\n",
    "        \"ì œ ìƒíƒœê°€ ì ì  ë” ë‚˜ë¹ ì ¸ì„œ ê°€ì¡±ë“¤ì—ê²Œ ì§ì´ ë ê¹Œ ë´ ê°€ì¥ ê±±ì •ì…ë‹ˆë‹¤. \"\n",
    "        \"ëŒ€í™”í•  ë•Œ ë‹¨ì–´ê°€ ì˜ ë– ì˜¤ë¥´ì§€ ì•Šì•„ì„œ ì‚¬ëŒë“¤ì´ ì´ìƒí•˜ê²Œ ìƒê°í• ê¹Œ ë´ ì°½í”¼í•˜ê¸°ë„ í•©ë‹ˆë‹¤.\"\n",
    "    )\n",
    "    with open(INPUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(demo_stt)\n",
    "\n",
    "out_path = summarise_from_txt_and_save(INPUT_TXT, OUTPUT_TXT,\n",
    "                                       guide_question_index=QUESTION_INDEX, session_id=SESSION_ID)\n",
    "print(f\"âœ… ì €ì¥ ì™„ë£Œ: {out_path}\")\n",
    "with open(out_path, 'r', encoding='utf-8') as f:\n",
    "    preview = f.read()\n",
    "print('------ ë¯¸ë¦¬ë³´ê¸° ------')\n",
    "print(preview[:1200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "3099cade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… (ì„ íƒ) CLI ë£¨í”„ ì¤€ë¹„ë¨ â€” í•„ìš” ì‹œ start_chat() ì‹¤í–‰\n"
     ]
    }
   ],
   "source": [
    "# part 5 (optional cli)\n",
    "#==========================================================\n",
    "# ğŸ“Œ ê°„ë‹¨ ëŒ€í™” ë£¨í”„ (ì˜¤í”„í† í”½ ì°¨ë‹¨ + ìš”ì•½)\n",
    "#==========================================================\n",
    "# ì‚¬ìš©ë²•:\n",
    "#  - ì²« ë°œí™”ëŠ” 4ê°œ ê³ ì • ì§ˆë¬¸ ì¤‘ í•˜ë‚˜ë¥¼ ì¶œë ¥\n",
    "#  - ì´í›„ ì‚¬ìš©ìì˜ ê¸´ ì‘ë‹µ(í…ìŠ¤íŠ¸)ì„ ë°›ì•„ ìš”ì•½ í…œí”Œë¦¿ìœ¼ë¡œ ì •ë¦¬\n",
    "\n",
    "def start_chat(guide_question_index: int = 3):\n",
    "    q = GUIDE_QUESTIONS[max(0, min(3, int(guide_question_index)))]\n",
    "    print(f\"[ìƒë‹´ ì±—ë´‡] {q}\")\n",
    "    print(\"[ì•ˆë‚´] ê¸´ ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš”. '/quit' ì…ë ¥ ì‹œ ì¢…ë£Œ.\")\n",
    "    while True:\n",
    "        user = input(\"\\n[ì‚¬ìš©ì] \").strip()\n",
    "        if user.lower() in {\"/quit\", \"quit\", \"exit\"}:\n",
    "            print(\"ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "            break\n",
    "        result = summarise_transcript(user, guide_question_index=guide_question_index)\n",
    "        print(\"\\n[ìš”ì•½]\\n\")\n",
    "        print(result)\n",
    "\n",
    "# start_chat(guide_question_index=3)  # â† í•„ìš” ì‹œ ì£¼ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰\n",
    "print(\"âœ… (ì„ íƒ) CLI ë£¨í”„ ì¤€ë¹„ë¨ â€” í•„ìš” ì‹œ start_chat() ì‹¤í–‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "fbcd085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìš”ì•½ ì €ì¥ ì™„ë£Œ: .\\summary_1.txt\n",
      "\n",
      "------ ìš”ì•½ ë¯¸ë¦¬ë³´ê¸° ------\n",
      "<ìš”ì•½>\n",
      "1. **ì£¼ ì¦ìƒ**\n",
      "- ë¬¼ê±´ì„ ìƒì–´ë²„ë¦¼\n",
      "- ì™¸ì¶œ ì‹œ ê¸¸ì„ ìƒì„ê¹Œ ë‘ë ¤ì›€\n",
      "- ì „í™” í†µí™” í›„ ê¸°ì–µ ìƒì‹¤\n",
      "- ë‹¨ì–´ê°€ ì˜ ë– ì˜¤ë¥´ì§€ ì•ŠìŒ\n",
      "\n",
      "2. **ìƒë‹´ë‚´ìš©**\n",
      "- ì•ˆê²½ì´ë‚˜ íœ´ëŒ€í°ì„ ë‘ê³  ì°¾ì§€ ëª»í•´ ê°€ì¡±ì—ê²Œ ìì£¼ ë¬¼ì–´ë´„.\n",
      "- í˜¼ì ì™¸ì¶œí•˜ëŠ” ê²ƒì´ ë¬´ì„­ê³  ê¸¸ì„ ìƒì„ê¹Œ ê±±ì •ë¨.\n",
      "- ì „í™” í†µí™”ë¥¼ í•˜ê³  ë‚˜ë©´ ëŒ€í™” ë‚´ìš©ì„ ê¸ˆë°© ìŠì–´ë²„ë¦¼.\n",
      "- ìì‹ ì˜ ìƒíƒœê°€ ë‚˜ë¹ ì ¸ ê°€ì¡±ì—ê²Œ ì§ì´ ë ê¹Œ ê±±ì •í•¨.\n",
      "- ëŒ€í™” ì¤‘ ë‹¨ì–´ê°€ ì˜ ìƒê°ë‚˜ì§€ ì•Šì•„ ì°½í”¼í•¨.\n",
      "\n",
      "3. **ì‹¬ë¦¬ìƒíƒœ**\n",
      "(ì—†ìŒ)\n",
      "\n",
      "4. **AI í•´ì„**\n",
      "í˜„ì¬ì˜ ì¦ìƒì€ ê²½ë„ì¸ì§€ì¥ì• (MCI)ì™€ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë©°, ì¡°ê¸° ì§„ë‹¨ê³¼ ì¹˜ë£Œê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "5. **ì£¼ì˜ì‚¬í•­**\n",
      "- ì¼ìƒì—ì„œ ë¬¼ê±´ì„ ë‘ëŠ” ìë¦¬ë¥¼ ì •í•´ë‘ê³ , ì™¸ì¶œ ì‹œì—ëŠ” ì•ˆì „í•œ ê²½ë¡œë¥¼ ë¯¸ë¦¬ ê³„íší•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
      "- ê°€ì¡±ê³¼ì˜ ì†Œí†µì„ í†µí•´ ì •ì„œì  ì§€ì§€ë¥¼ ë°›ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
      "- ì¦ìƒì´ ì§€ì†ë˜ê±°ë‚˜ ì•…í™”ë  ê²½ìš° ì „ë¬¸ì˜ì™€ ìƒë‹´í•˜ì—¬ ê²€ì§„ì„ ë°›ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# part 6\n",
    "#==========================================================\n",
    "# ğŸ“Œ íŒŒì¼ ì…ë ¥ ì „ìš© ì‹¤í–‰ (ì˜¤í”„í† í”½ ì°¨ë‹¨ + ìš”ì•½ ì €ì¥)\n",
    "#   - ë°˜ë³µë¬¸(ëŒ€í™” ë£¨í”„) ì œê±°\n",
    "#   - ./script_1.txt â†’ ìš”ì•½ â†’ ./summary_1_.txt (ë˜ëŠ” ìë™ íŒŒì¼ëª…)\n",
    "#==========================================================\n",
    "import os\n",
    "\n",
    "# [ì„¤ì •ê°’]\n",
    "INPUT_TXT            = \"./script_1.txt\"     # âœ… ì‹¤ì œ STT í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ\n",
    "# INPUT_TXT            = \"./ê°œì†Œë¦¬4.txt\"     # âœ… ì‹¤ì œ STT í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ\n",
    "# INPUT_TXT            = \"./ë¥´ì„¸ë¼í•Œ_smart.txt\"     # âœ… ì‹¤ì œ STT í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ\n",
    "OUTPUT_DIR           = \".\"                  # ì €ì¥ í´ë”\n",
    "USE_AUTO_NAMING      = False                # True: summary_<ì›ë³¸íŒŒì¼ëª…>.txt, False: ê³ ì • íŒŒì¼ëª… ì‚¬ìš©\n",
    "OUTPUT_FIXED_NAME    = \"summary_1.txt\"     # USE_AUTO_NAMING=Falseì¼ ë•Œ ì‚¬ìš©\n",
    "GUIDE_QUESTION_INDEX = 1                    # 0~3 (Q1~Q4)\n",
    "SESSION_ID           = \"file-run-session-1\" # ì„¸ì…˜ ID\n",
    "SKIP_OFFTOPIC_SAVE   = True                 # ì˜¤í”„í† í”½ì´ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "# 1) ê°€ì´ë“œ ì§ˆë¬¸ì„ ë˜ì§„ ê²ƒìœ¼ë¡œ ê°„ì£¼ â†’ ì„¸ì…˜ prior boost\n",
    "mark_guide_question_shown(SESSION_ID)\n",
    "\n",
    "# 2) ì…ë ¥ íŒŒì¼ í™•ì¸\n",
    "if not os.path.exists(INPUT_TXT):\n",
    "    raise FileNotFoundError(f\"ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {INPUT_TXT}\")\n",
    "\n",
    "# 3) ìš”ì•½ ìƒì„± (ì˜¤í”„í† í”½/ë³´ë¥˜ ì•ˆë‚´ í¬í•¨)\n",
    "summary_only = summarise_from_txt(\n",
    "    INPUT_TXT,\n",
    "    guide_question_index=GUIDE_QUESTION_INDEX,\n",
    "    session_id=SESSION_ID,\n",
    ")\n",
    "\n",
    "# 4) ì˜¤í”„í† í”½ ì²˜ë¦¬(ì €ì¥ ì—¬ë¶€)\n",
    "if summary_only.strip() == OFF_TOPIC_MESSAGE and SKIP_OFFTOPIC_SAVE:\n",
    "    print(\"âš ï¸ ì˜¤í”„í† í”½ìœ¼ë¡œ íŒì •ë˜ì–´ ì €ì¥ì„ ìƒëµí•©ë‹ˆë‹¤.\")\n",
    "    print(\"ë©”ì‹œì§€:\", OFF_TOPIC_MESSAGE)\n",
    "else:\n",
    "    # 5) ì¶œë ¥ ê²½ë¡œ ê²°ì •\n",
    "    if USE_AUTO_NAMING:\n",
    "        base = os.path.splitext(os.path.basename(INPUT_TXT))[0]\n",
    "        out_name = f\"summary_{base}.txt\"\n",
    "    else:\n",
    "        out_name = OUTPUT_FIXED_NAME\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    out_path = os.path.join(OUTPUT_DIR, out_name)\n",
    "\n",
    "    # 6) ì €ì¥\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(summary_only.strip() + \"\\n\")\n",
    "\n",
    "    print(f\"âœ… ìš”ì•½ ì €ì¥ ì™„ë£Œ: {out_path}\\n\")\n",
    "    # 7) ë¯¸ë¦¬ë³´ê¸°    preview = summary_only.strip()\n",
    "    print(\"------ ìš”ì•½ ë¯¸ë¦¬ë³´ê¸° ------\")\n",
    "    print(preview[:1200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d63328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a3979d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f9d726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
