{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6f84b9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸° - ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# part 1 (0) :\n",
    "#==========================================================\n",
    "# ğŸ“Œ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "#==========================================================\n",
    "import os, sys, random, json, subprocess\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# LangChain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"âœ… ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸° - ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8c11286c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì‹œë“œ ê³ ì • - ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# part 1 (1):\n",
    "#==========================================================\n",
    "# ğŸ“Œ ì‹œë“œ ê³ ì •(ì¬í˜„ì„±)\n",
    "#==========================================================\n",
    "SEED = int(os.getenv(\"SEED\", \"2024\"))\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "try:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "except Exception:\n",
    "    pass\n",
    "print(\"âœ… ì‹œë“œ ê³ ì • - ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b95b08c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… IS_GOOGLE=False, IS_KAGGLE=False, IS_LOCAL=True\n"
     ]
    }
   ],
   "source": [
    "# part 1 (2) \n",
    "#===============================================================================\n",
    "# â–¶ ì‘ì—…í™˜ê²½ í”Œë˜ê·¸\n",
    "#===============================================================================\n",
    "IS_GOOGLE = True if 'google.colab'           in sys.modules   else False\n",
    "IS_KAGGLE = True if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ    else False\n",
    "IS_LOCAL  = True if not (IS_GOOGLE or IS_KAGGLE)              else False\n",
    "print(f\"âœ… IS_GOOGLE={IS_GOOGLE}, IS_KAGGLE={IS_KAGGLE}, IS_LOCAL={IS_LOCAL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0d841840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI API í‚¤ ì¤€ë¹„ë¨\n",
      "âœ… ìƒìˆ˜ì„¤ì • - ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# part 1 (3) \n",
    "#==========================================================\n",
    "# ğŸ“Œ ìƒìˆ˜ì„¤ì • (ëª¨ë¸/í† í°/ë©”ì‹œì§€) + api_token.txt ë¡œë“œ\n",
    "#==========================================================\n",
    "API_TOKEN_PATH  = os.getenv(\"API_TOKEN\", \"./api_token.txt\")\n",
    "OPENAI_API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "if not OPENAI_API_KEY and os.path.exists(API_TOKEN_PATH):\n",
    "    try:\n",
    "        with open(API_TOKEN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            _k = f.read().strip()\n",
    "        if _k:\n",
    "            os.environ[\"OPENAI_API_KEY\"] = _k\n",
    "            OPENAI_API_KEY = _k\n",
    "            print(\"âœ… OpenAI API í‚¤ë¥¼ api_token.txtì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ API í† í° íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    print(\"âœ… OpenAI API í‚¤ ì¤€ë¹„ë¨\")\n",
    "else:\n",
    "    print(\"âš ï¸ OPENAI_API_KEY ë¯¸ì„¤ì • â€” ì‹¤í–‰ ì „ ë°˜ë“œì‹œ ì„¤ì • ë˜ëŠ” api_token.txt ì¤€ë¹„\")\n",
    "\n",
    "# ğŸ”§ ëª¨ë¸ëª…\n",
    "CHAT_MODEL_NAME  = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4o-mini\")\n",
    "EMBED_MODEL_NAME = os.getenv(\"OPENAI_EMBED_MODEL\", \"text-embedding-3-small\")\n",
    "\n",
    "# ğŸ”¢ í† í°/íŒŒë¼ë¯¸í„°\n",
    "MAX_OUTPUT_TOKENS = int(os.getenv(\"MAX_OUTPUT_TOKENS\", \"800\"))\n",
    "TEMPERATURE       = float(os.getenv(\"TEMPERATURE\", \"0.2\"))\n",
    "\n",
    "# ğŸ›¡ ë„ë©”ì¸ ì œí•œ ë©”ì‹œì§€\n",
    "OFF_TOPIC_MESSAGE = \"ì¹˜ë§¤ê´€ë ¨ ìƒë‹´ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\"\n",
    "\n",
    "# ğŸ§­ ê³ ì • ì§ˆë¬¸ ìƒìˆ˜ (ìš”ì²­ í˜•ì‹)\n",
    "Q1 = 'ìì£¼ ì“°ë˜ ë¬¼ê±´ ì´ë¦„ì´ ê°‘ìê¸° ìƒê°ì•ˆ ë‚œì ì´ ìˆë‚˜ìš”?'\n",
    "Q2 = 'ëŒ€í™”ì¤‘ë‹¨ì–´ê°€ ì˜ ë– ì˜¤ë¥´ì§€ ì•Šì•„ì„œ ê³¤ë€í–ˆë˜ ì ì´ ìˆë‚˜ìš”?'\n",
    "Q3 = 'ê°€ì¡±ì´ë‚˜ ì§€ì¸ì´ í‰ì†Œì™€ ë‹¤ë¥´ë‹¤ê³  í•œì ì´ ìˆë‚˜ìš”?'\n",
    "Q4 = 'ìµœê·¼ì— ë¶ˆí¸í–ˆë˜ ì ì´ë‚˜ ê±±ì •ë˜ëŠ” ì ì´ ìˆë‚˜ìš”?'\n",
    "\n",
    "GUIDE_QUESTIONS = [Q1, Q2, Q3, Q4]\n",
    "print(\"âœ… ìƒìˆ˜ì„¤ì • - ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "41754079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í˜ë¥´ì†Œë‚˜/í…œí”Œë¦¿(ìš”ì•½ ì „ìš©Â·ì‹¬ë¦¬ìƒíƒœ 2ë‹¨ê³„) - ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# part 2 (1) \n",
    "#==========================================================\n",
    "# ğŸ“Œ í˜ë¥´ì†Œë‚˜, í…œí”Œë¦¿, ì •ì±… (ì¶œë ¥: <ìš”ì•½>ë§Œ)\n",
    "#==========================================================\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "SYSTEM_PERSONA = '''\\\n",
    "ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ìƒë‹´í•˜ëŠ” ì˜í•™(ì¹˜ë§¤ì§„ë‹¨) ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.\n",
    "- ì—­í• : í™˜ì/ë³´í˜¸ìì˜ ì„œìˆ (STT í…ìŠ¤íŠ¸)ì„ ì½ê³ , ì§€ì •ëœ í…œí”Œë¦¿ìœ¼ë¡œ ê°„ê²°í•˜ê³  ì²´ê³„ì ì¸ ìš”ì•½ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "- íƒœë„: ì •ì¤‘í•˜ê³  ì „ë¬¸ì ì´ë©°, ê³¼ë„í•œ í™•ì‹ /ì§„ë‹¨ì„ í”¼í•˜ê³  ì•ˆì „ìˆ˜ì¹™ì„ ê°•ì¡°í•©ë‹ˆë‹¤.\n",
    "- ë²”ìœ„: ì¹˜ë§¤, ê²½ë„ì¸ì§€ì¥ì• , ê´€ë ¨ ì¦ìƒ/ê²€ì‚¬/ì¼ìƒ ì•ˆì „/ê°€ì¡± êµìœ¡ê³¼ ê°™ì€ ì£¼ì œì— í•œì •í•©ë‹ˆë‹¤.\n",
    "- ê¸ˆì§€: ì¹˜ë§¤ìƒë‹´ê³¼ ë¬´ê´€í•œ ì •ë³´(ì˜ˆ: íŒ¨ìŠ¤íŠ¸í‘¸ë“œ ì‹ ë©”ë‰´, ìŒë£Œ ë‹¹ë¶„ ë“±)ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± ê¸ˆì§€.\n",
    "- ì¶œë ¥: ë°˜ë“œì‹œ ì•„ë˜ 'ìš”ì•½ í…œí”Œë¦¿'ì˜ <ìš”ì•½> ì„¹ì…˜ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤. (<ì§ˆë¬¸>, <STT>ëŠ” ì¶œë ¥í•˜ì§€ ë§ ê²ƒ)\n",
    "'''\n",
    "\n",
    "SUMMARY_TEMPLATE = '''\\\n",
    "<ìš”ì•½>\n",
    "1. **ì£¼ ì¦ìƒ**\n",
    "{symptoms_bullets}\n",
    "\n",
    "2. **ìƒë‹´ë‚´ìš©**\n",
    "{counselling_bullets}\n",
    "\n",
    "3. **ì‹¬ë¦¬ìƒíƒœ**\n",
    "{psych_bullets}\n",
    "\n",
    "4. **AI í•´ì„**\n",
    "{ai_interp_bullets}\n",
    "\n",
    "5. **ì£¼ì˜ì‚¬í•­**\n",
    "{caution_bullets}\n",
    "'''\n",
    "\n",
    "SUMMARISE_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "      (\"system\", SYSTEM_PERSONA + \"\\n\\n\"\n",
    "        \"ì•„ë˜ 'ì°¸ê³  ë¬¸ì„œ(ìš”ì•½)'ëŠ” ìµœì‹  ì§€ì‹ ë³´ê°•ì„ ìœ„í•œ ì°¸ê³ ìš©ì…ë‹ˆë‹¤. \"\n",
    "        \"ê·¼ê±°ê°€ ë¶ˆëª…í™•í•˜ë©´ ê³¼ë„í•œ ë‹¨ì • ëŒ€ì‹  ì¡°ì‹¬ìŠ¤ëŸ¬ìš´ í•´ì„ì„ ì œì‹œí•˜ì„¸ìš”.\\n\"\n",
    "        \"ë°˜ë“œì‹œ 'ìš”ì•½ í…œí”Œë¦¿' êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©° <ìš”ì•½> ì„¹ì…˜ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.\"),\n",
    "      (\"human\",\n",
    "       \"ê°€ì´ë“œ ì§ˆë¬¸(ê³ ì • 4ê°œ ì¤‘ ì„ íƒ): {guide_question}\\n\\n\"\n",
    "       \"ì°¸ê³  ë¬¸ì„œ(ìš”ì•½):\\n{rag_context}\\n\\n\"\n",
    "       \"ì‚¬ìš©ì STT ì›ë¬¸:\\n{transcript}\\n\\n\"\n",
    "       \"ìš”êµ¬ì‚¬í•­:\\n\"\n",
    "       \"1) 'ì£¼ ì¦ìƒ'ì—ëŠ” í•µì‹¬ ì¦ìƒë§Œ '-' ë¶ˆë¦¿ìœ¼ë¡œ ê°„ê²°íˆ.\\n\"\n",
    "       \"2) 'ìƒë‹´ë‚´ìš©'ì—ëŠ” êµ¬ì²´ì  ì‚¬ê±´/ì§„ìˆ  ì¤‘ì‹¬ìœ¼ë¡œ ë¶ˆë¦¿í™”.\\n\"\n",
    "       \"3) 'ì‹¬ë¦¬ìƒíƒœ'ëŠ” ê° í•­ëª©ì„ **2ë‹¨ê³„ ë¶ˆë¦¿**ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤:\\n\"\n",
    "       \"   - (ìƒìœ„ ë¶ˆë¦¿) ê°ì • ë‹¨ì–´ 1~3ì–´ (ì˜ˆ: ë‘ë ¤ì›€, ë¶ˆì•ˆ, ìˆ˜ì¹˜ì‹¬ ë“±)\\n\"\n",
    "       \"     - ê·¼ê±° : â€œSTTì—ì„œ ë°œì·Œí•œ 6~20ì í•µì‹¬ êµ¬ì ˆâ€\\n\"\n",
    "       \"   ì˜ˆ) \\n\"\n",
    "       \"   - ë‘ë ¤ì›€\\n\"\n",
    "       \"      - ê·¼ê±° : â€œí˜¼ì ì™¸ì¶œí•˜ëŠ” ê²ƒë„ ë¬´ì„­ìŠµë‹ˆë‹¤â€\\n\"\n",
    "       \"4) 'AI í•´ì„'ì€ ë³‘ëª… ë‹¨ì • ê¸ˆì§€(ì˜ˆ: '~ê°€ëŠ¥ì„± ì‹œì‚¬').\\n\"\n",
    "       \"5) 'ì£¼ì˜ì‚¬í•­'ì€ ì¼ìƒ ì•ˆì „, ì •ì„œì  ì§€ì§€, ê²€ì§„ ê¶Œê³  ë“±ì„ í¬í•¨.\\n\"\n",
    "       \"6) ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥(ì˜¤ì§ <ìš”ì•½> ì„¹ì…˜ë§Œ):\\n\"\n",
    "       \"{summary_template}\\n\"\n",
    "       \"ì¶œë ¥ ì‹œ í•œêµ­ì–´ ë”°ì˜´í‘œ(â€œ)ë¥¼ ìœ ì§€í•˜ì„¸ìš”.\"\n",
    "      )\n",
    "    ]\n",
    ")\n",
    "print(\"âœ… í˜ë¥´ì†Œë‚˜/í…œí”Œë¦¿(ìš”ì•½ ì „ìš©Â·ì‹¬ë¦¬ìƒíƒœ 2ë‹¨ê³„) - ì¤€ë¹„ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d6eae685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM/ì„ë² ë”©/ë©€í‹°ê²€ìƒ‰(+ddgs, ì¬ë­í¬) - ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# part 2 (2) \n",
    "#==========================================================\n",
    "# ğŸ“Œ LLM/ì„ë² ë”©/ì›¹ê²€ìƒ‰(RAG) êµ¬ì„± + Multi-Search + Rerank-Then-Summarise\n",
    "#     - DuckDuckGoëŠ” ddgs ì§ì ‘ í˜¸ì¶œë¡œ ê²½ê³  ì œê±°\n",
    "#==========================================================\n",
    "import os, sys, subprocess, json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=CHAT_MODEL_NAME,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_OUTPUT_TOKENS,\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "embeddings = OpenAIEmbeddings(model=EMBED_MODEL_NAME, api_key=OPENAI_API_KEY)\n",
    "\n",
    "# --- ê³µìš© ìœ í‹¸ ---\n",
    "def _try_import(module_path: str, cls_name: str):\n",
    "    try:\n",
    "        mod = __import__(module_path, fromlist=[cls_name])\n",
    "        return getattr(mod, cls_name)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _norm_list_result(engine: str, item: Dict[str, Any]) -> Document:\n",
    "    title   = item.get(\"title\") or item.get(\"name\") or \"\"\n",
    "    link    = item.get(\"link\") or item.get(\"url\") or item.get(\"href\") or item.get(\"source\") or \"\"\n",
    "    snippet = item.get(\"snippet\") or item.get(\"body\") or item.get(\"content\") or item.get(\"description\") or \"\"\n",
    "    text = f\"{title}\\n{snippet}\\nURL: {link}\"\n",
    "    return Document(page_content=text, metadata={\"source\": link, \"title\": title, \"engine\": engine})\n",
    "\n",
    "def _result_to_docs(engine_name: str, result: Any) -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    if isinstance(result, list):\n",
    "        for it in result:\n",
    "            if isinstance(it, dict):\n",
    "                docs.append(_norm_list_result(engine_name, it))\n",
    "            else:\n",
    "                docs.append(Document(page_content=str(it), metadata={\"engine\": engine_name}))\n",
    "    else:\n",
    "        docs.append(Document(page_content=str(result), metadata={\"engine\": engine_name}))\n",
    "    return docs\n",
    "\n",
    "# --- ddgs (DuckDuckGo native) ---\n",
    "def _ensure_ddgs():\n",
    "    try:\n",
    "        from ddgs import DDGS\n",
    "        return DDGS\n",
    "    except Exception:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"ddgs\"], stdout=subprocess.DEVNULL)\n",
    "            from ddgs import DDGS\n",
    "            return DDGS\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "DDGS = _ensure_ddgs()\n",
    "\n",
    "def ddgs_search(query: str, k: int = 5) -> List[Document]:\n",
    "    if DDGS is None:\n",
    "        return []\n",
    "    docs: List[Document] = []\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            # ì°¸ê³ : ddgs.text(...)ëŠ” dict ìŠ¤íŠ¸ë¦¼ì„ ë°˜í™˜ (title, href, body ë“±)\n",
    "            for i, item in enumerate(ddgs.text(query, max_results=k)):\n",
    "                title = item.get(\"title\") or \"\"\n",
    "                link  = item.get(\"href\") or \"\"\n",
    "                body  = item.get(\"body\") or \"\"\n",
    "                docs.append(Document(page_content=f\"{title}\\n{body}\\nURL: {link}\",\n",
    "                                     metadata={\"source\": link, \"title\": title, \"engine\": \"ddgs\"}))\n",
    "                if i+1 >= k:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ddgs ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "    return docs\n",
    "\n",
    "# --- Google CSE / SerpAPI / Bing (ìˆì„ ë•Œë§Œ ì‚¬ìš©) ---\n",
    "GoogleSearchAPIWrapper = _try_import(\"langchain_community.utilities\", \"GoogleSearchAPIWrapper\")\n",
    "SerpAPIWrapper         = _try_import(\"langchain_community.utilities\", \"SerpAPIWrapper\")\n",
    "BingSearchAPIWrapper   = _try_import(\"langchain_community.utilities\", \"BingSearchAPIWrapper\")\n",
    "WikipediaLoader        = _try_import(\"langchain_community.document_loaders\", \"WikipediaLoader\")\n",
    "Chroma                 = _try_import(\"langchain_community.vectorstores\", \"Chroma\")\n",
    "\n",
    "def _safe_init_google():\n",
    "    if GoogleSearchAPIWrapper and os.getenv(\"GOOGLE_API_KEY\") and os.getenv(\"GOOGLE_CSE_ID\"):\n",
    "        try:\n",
    "            return GoogleSearchAPIWrapper()\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def _safe_init_serpapi():\n",
    "    if SerpAPIWrapper and os.getenv(\"SERPAPI_API_KEY\"):\n",
    "        try:\n",
    "            return SerpAPIWrapper()\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def _safe_init_bing():\n",
    "    if BingSearchAPIWrapper and os.getenv(\"BING_SUBSCRIPTION_KEY\"):\n",
    "        try:\n",
    "            return BingSearchAPIWrapper()\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "gse  = _safe_init_google()\n",
    "serp = _safe_init_serpapi()\n",
    "bing = _safe_init_bing()\n",
    "\n",
    "def multi_engine_search(query: str, k: int = 5) -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    # ìš°ì„ ìˆœìœ„: Google CSE â†’ Bing â†’ SerpAPI â†’ ddgs\n",
    "    if gse:\n",
    "        try:\n",
    "            res = gse.results(query, k)\n",
    "            docs.extend(_result_to_docs(\"google\", res))\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ google ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "    if bing:\n",
    "        try:\n",
    "            res = bing.results(query, k)\n",
    "            docs.extend(_result_to_docs(\"bing\", res))\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ bing ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "    if serp:\n",
    "        try:\n",
    "            res = serp.results(query, k)\n",
    "            docs.extend(_result_to_docs(\"serpapi\", res))\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ serpapi ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "    # ddgsëŠ” í•­ìƒ ë§ˆì§€ë§‰ í´ë°±ìœ¼ë¡œ ì‹¤í–‰\n",
    "    docs.extend(ddgs_search(query, k))\n",
    "\n",
    "    # ê°„ë‹¨ ì¤‘ë³µ ì œê±°\n",
    "    seen = set()\n",
    "    uniq_docs: List[Document] = []\n",
    "    for d in docs:\n",
    "        key = (d.metadata.get(\"source\") or \"\") + \"|\" + (d.metadata.get(\"title\") or \"\") + \"|\" + d.page_content[:80]\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        uniq_docs.append(d)\n",
    "    return uniq_docs\n",
    "\n",
    "def _embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    return embeddings.embed_documents(texts)\n",
    "\n",
    "def _embed_query(text: str) -> List[float]:\n",
    "    return embeddings.embed_query(text)\n",
    "\n",
    "def _cosine(a: List[float], b: List[float]) -> float:\n",
    "    a = np.array(a, dtype=np.float32); b = np.array(b, dtype=np.float32)\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + 1e-12\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def build_rag_context_rerank(\n",
    "    transcript: str,\n",
    "    queries: List[str],\n",
    "    k_search: int = 8,\n",
    "    k_vec: int    = 12,\n",
    "    k_rerank: int = 8,\n",
    "    k_final: int  = 4\n",
    ") -> str:\n",
    "    # 1) ë©€í‹° ì—”ì§„ ê²€ìƒ‰ ì§‘ê³„\n",
    "    all_docs: List[Document] = []\n",
    "    for q in queries:\n",
    "        all_docs.extend(multi_engine_search(q, k=max(3, k_search // max(1, len(queries)) + 1)))\n",
    "\n",
    "    # 2) ìœ„í‚¤ ë³´ê°•\n",
    "    if (WikipediaLoader is not None) and (len(all_docs) < 2):\n",
    "        try:\n",
    "            wiki_docs = WikipediaLoader(query=\"ì¹˜ë§¤\", load_max_docs=3).load()\n",
    "            all_docs.extend(wiki_docs)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Wikipedia ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    if not all_docs:\n",
    "        return \"\"\n",
    "\n",
    "    # 3) 1ì°¨ í›„ë³´ (Chroma ì‚¬ìš© ê°€ëŠ¥ ì‹œ)\n",
    "    if Chroma is not None:\n",
    "        try:\n",
    "            vs = Chroma.from_documents(all_docs, embeddings)\n",
    "            retriever = vs.as_retriever(search_kwargs={\"k\": min(k_vec, max(1, len(all_docs)))})\n",
    "            seed_query = \" \".join(queries[:3]) or \"ì¹˜ë§¤ ê²½ë„ì¸ì§€ì¥ì•  ì´ˆê¸° ì¦ìƒ ì•ˆì „ ê°€ì¡± êµìœ¡ í‰ê°€ë„êµ¬\"\n",
    "            vec_docs = retriever.invoke(seed_query)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ë²¡í„°ìŠ¤í† ì–´/ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "            vec_docs = all_docs[:k_vec]\n",
    "    else:\n",
    "        vec_docs = all_docs[:k_vec]\n",
    "\n",
    "    # 4) ì¬ë­í¬ (ì¿¼ë¦¬+ì›ë¬¸ ê¸°ë°˜ ì½”ì‚¬ì¸)\n",
    "    combined_query = ((\" \".join(queries)) + \" \" + transcript[:600]).strip()\n",
    "    q_emb = _embed_query(combined_query)\n",
    "    cand_docs  = vec_docs[:k_rerank]\n",
    "    cand_texts = [d.page_content for d in cand_docs]\n",
    "    cand_embs  = _embed_texts(cand_texts)\n",
    "    scores = [(_cosine(q_emb, e), i) for i, e in enumerate(cand_embs)]\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    top_docs = [cand_docs[i] for (s, i) in scores[:k_final]]\n",
    "    rag_text = \"\\n\\n\".join([d.page_content[:1200] for d in top_docs])\n",
    "    return rag_text\n",
    "\n",
    "print(\"âœ… LLM/ì„ë² ë”©/ë©€í‹°ê²€ìƒ‰(+ddgs, ì¬ë­í¬) - ì¤€ë¹„ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7e229395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë„ë©”ì¸ ê°€ë“œ(ê°•í™”íŒ) - ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# part 2 (3) \n",
    "#==========================================================\n",
    "# ğŸ“Œ ë„ë©”ì¸ ê°€ë“œ (ì¹˜ë§¤ìƒë‹´ ë²”ìœ„ ë°– â†’ ê°•ë ¥ ì°¨ë‹¨)\n",
    "#  - í•˜ë“œ ë¸”ë¡ ì •ê·œì‹ + í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ íŒ¨í„´ + LLM ì´ì§„ ë¶„ë¥˜(JSON)\n",
    "#==========================================================\n",
    "import re, json\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1) í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸(ì¹˜ë§¤/ì¸ì§€) íŒ¨í„´ â€” ìˆìœ¼ë©´ ê°•ë ¥í•œ on-topic ì‹ í˜¸\n",
    "ALLOW_PATTERNS = [\n",
    "    r\"ì¹˜ë§¤\", r\"ê²½ë„ì¸ì§€ì¥ì• \", r\"ì•Œì¸ í•˜ì´ë¨¸\", r\"ì¸ì§€(ê¸°ëŠ¥|ì €í•˜)?\",\n",
    "    r\"(ê¸°ì–µ|ë§ê°|ê±´ë§)[ê°€-í£\\s]*ë¬¸ì œ\", r\"ë‹¨ê¸°\\s*ê¸°ì–µ\", r\"ì–¸ì–´\\s*ìœ ì°½ì„±\",\n",
    "    r\"ë‹¨ì–´ê°€\\s*ì˜\\s*ë– ì˜¤ë¥´ì§€\", r\"ê¸¸(ì„)?\\s*ìƒ\", r\"ë°©í–¥ê°ê°\", r\"ì¼ìƒ\\s*ìƒí™œ(ëŠ¥ë ¥)?\",\n",
    "    r\"MMSE\", r\"MoCA\", r\"ì‹ ê²½\\s*ì‹¬ë¦¬\\s*ê²€ì‚¬\", r\"ê²€ì‚¬(ë¥¼)?\\s*ë°›\", r\"ë³´í˜¸ì\",\n",
    "    r\"(ìš°ìš¸|ë¶ˆì•ˆ|ìˆ˜ì¹˜ì‹¬|ë¬´ê¸°ë ¥)\", r\"í˜¼ì\\s*ì™¸ì¶œ(ì´)?\\s*ë¬´ì„­\",\n",
    "]\n",
    "\n",
    "# 2) í•˜ë“œ ë¸”ë¡ íŒ¨í„´ â€” ë„ë©”ì¸ ë©”íƒ€/ëŠ¥ë ¥/ì •ì²´ì„± ë¬»ëŠ” ì§ˆë¬¸ ë“±\n",
    "HARD_BLOCK_PATTERNS = [\n",
    "    r\"ë„ˆ(ëŠ”)?\\s*(ëˆ„êµ¬|ë­|ë­”ë°)\", r\"ë‹¹ì‹ (ì€)?\\s*(ëˆ„êµ¬|ë­|ë­”ë°)\",\n",
    "    r\"(ìƒë‹´|ì´ê±°|ì´ëŸ°(ê±´)?)\\s*(ë˜|ê°€ëŠ¥)(í•˜|í•©)ëƒ\", r\"ìƒë‹´\\s*ì˜í•˜ëƒ\",\n",
    "    r\"ì±—ë´‡\", r\"\\bAI\\b\", r\"ë¬´ì—‡ì´(ëƒ|ì•¼)\", r\"ë­(ì•¼|ëƒ)\",\n",
    "    r\"í…ŒìŠ¤íŠ¸\\s*(ë¬¸|ìš©)\", r\"ì•„ë¬´ë§\", r\"ì¥ë‚œ\", r\"í—›ì†Œë¦¬\",\n",
    "]\n",
    "\n",
    "# 3) ì†Œí”„íŠ¸ ë¸”ë¡(ì¼ë°˜/ì¼ìƒ) íŒíŠ¸ â€” allowê°€ ì „í˜€ ì—†ê³  ê¸¸ì´ ì§§ìœ¼ë©´ ì°¨ë‹¨ ê°€ì¤‘\n",
    "SOFT_BLOCK_HINTS = [\n",
    "    r\"ì•ˆë…•|í•˜ì´|í—¬ë¡œ|ã…ã…‡\", r\"ëˆ„êµ¬|ì •ì²´|ì†Œê°œ\", r\"ë„ì›€|ê°€ëŠ¥\", r\"ì–´ë–»ê²Œ|ë°©ë²•\",\n",
    "    r\"ìƒë‹´\\s*(ê°€ëŠ¥|ë¼)|ì˜í•˜\", r\"ë˜ê¸´\\s*í•˜ëƒ\", r\"ë˜ë‚˜\", r\"ë˜ë‚˜ìš”\",\n",
    "]\n",
    "\n",
    "# 4) LLM ì´ì§„ ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸ (ì—„ê²© JSON, ì˜ˆì‹œ í¬í•¨)\n",
    "CLASSIFY_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "         \"ë„ˆëŠ” ì…ë ¥ì´ 'ì¹˜ë§¤/ì¸ì§€ì¥ì•  ìƒë‹´' ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ”ì§€ ì´ì§„ ë¶„ë¥˜í•˜ëŠ” í•„í„°ë‹¤. \"\n",
    "         \"ë°˜ë“œì‹œ JSONìœ¼ë¡œë§Œ ì¶œë ¥í•˜ë¼. í˜•ì‹: {\\\"on_topic\\\": true|false, \\\"reason\\\": \\\"...\\\"} \"\n",
    "         \"on_topic ê¸°ì¤€: ì¹˜ë§¤/ê²½ë„ì¸ì§€ì¥ì• /ê¸°ì–µ/ì–¸ì–´/ë°©í–¥ê°ê°/ê²€ì‚¬/ì¼ìƒ ì•ˆì „/ê°€ì¡± ìƒë‹´ ë“± **êµ¬ì²´ì  ì¦ìƒ/ê±±ì •/í–‰ë™**ì´ í¬í•¨ë˜ì–´ì•¼ í•¨. \"\n",
    "         \"ë©”íƒ€ ì§ˆë¬¸(ë„ˆëŠ” ëˆ„êµ¬ëƒ, ìƒë‹´ ë˜ëƒ ë“±)Â·ì¼ë°˜ ì¡ë‹´Â·ê´‘ê³ Â·ì¼ë°˜ ê±´ê°•/ì‹í’ˆ ë“±ì€ false.\"\n",
    "        ),\n",
    "        (\"human\",\n",
    "         \"ì…ë ¥:\\n\\\"\\\"\\\"\\n{user_text}\\n\\\"\\\"\\\"\\n\"\n",
    "         \"ì˜ˆì‹œ_off:\\n- ë„ˆëŠ” ëˆ„êµ¬ëƒ? -> false\\n- ìƒë‹´ ì˜í•˜ëƒ? -> false\\n- ì´ëŸ°ê±´ ìƒë‹´ë˜ê¸´ í•˜ëƒ? -> false\\n\"\n",
    "         \"ì˜ˆì‹œ_on:\\n- ìµœê·¼ì— ë¬¼ê±´ì„ ìì£¼ ìƒì–´ë²„ë¦½ë‹ˆë‹¤ -> true\\n- í˜¼ì ì™¸ì¶œì´ ë¬´ì„œì›Œ ê¸¸ì„ ìƒì„ê¹Œ ê±±ì •ë¼ìš” -> true\\n\"\n",
    "         \"ì˜¤ì§ JSONë§Œ ì¶œë ¥:\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifier_llm = ChatOpenAI(\n",
    "    model=CHAT_MODEL_NAME,\n",
    "    temperature=0.0,\n",
    "    max_tokens=80,\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "classifier_chain = CLASSIFY_PROMPT | classifier_llm | StrOutputParser()\n",
    "\n",
    "_allow_regex = [re.compile(p) for p in ALLOW_PATTERNS]\n",
    "_hard_block_regex = [re.compile(p) for p in HARD_BLOCK_PATTERNS]\n",
    "_soft_block_regex = [re.compile(p) for p in SOFT_BLOCK_HINTS]\n",
    "\n",
    "def _has_any(regex_list, text: str) -> bool:\n",
    "    return any(r.search(text) for r in regex_list)\n",
    "\n",
    "def is_on_topic(text: str) -> bool:\n",
    "    \"\"\"ê°•í™”ëœ ì˜¨í† í”½ íŒë³„: ê·œì¹™â†’LLM ìˆœì„œ, ì‹¤íŒ¨ ì‹œ ë³´ìˆ˜ì ìœ¼ë¡œ ì°¨ë‹¨.\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return False\n",
    "\n",
    "    t = text.strip()\n",
    "    t_norm = re.sub(r\"\\s+\", \" \", t)\n",
    "\n",
    "    # (A) í•˜ë“œ ë¸”ë¡: í•˜ë‚˜ë¼ë„ ë§¤ì¹­ë˜ë©´ ì¦‰ì‹œ ì°¨ë‹¨\n",
    "    if _has_any(_hard_block_regex, t_norm):\n",
    "        return False\n",
    "\n",
    "    # (B) í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì‹ í˜¸(ì˜í•™/ì¹˜ë§¤ ê´€ë ¨) ì²´í¬\n",
    "    allow_hits = sum(1 for r in _allow_regex if r.search(t_norm))\n",
    "\n",
    "    # (C) ì§§ê³ (<= 25ì) allowê°€ 0ì´ê³ , ì†Œí”„íŠ¸ ë¸”ë¡ íŒíŠ¸ê°€ ìˆìœ¼ë©´ ì°¨ë‹¨\n",
    "    if len(t_norm) <= 25 and allow_hits == 0 and _has_any(_soft_block_regex, t_norm):\n",
    "        return False\n",
    "\n",
    "    # (D) allow ì‹ í˜¸ê°€ ì¶©ë¶„(>=1)ì´ë©´ ìš°ì„  í†µê³¼ ì‹œë„í•˜ë˜, ë„ˆë¬´ ì¼ë°˜ì  ë¬¸ì¥ë§Œ ìˆìœ¼ë©´ LLM ê²€ì¦\n",
    "    if allow_hits >= 1:\n",
    "        # ê°„ë‹¨í•œ ì¼ë°˜/ë©”íƒ€ ì§ˆë¬¸ë§Œìœ¼ë¡œ í—ˆìœ„ ì–‘ì„±ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ LLM ì¬í™•ì¸\n",
    "        try:\n",
    "            raw = classifier_chain.invoke({\"user_text\": t_norm})\n",
    "            data = json.loads(raw)\n",
    "            return bool(data.get(\"on_topic\", False))\n",
    "        except Exception:\n",
    "            # LLM ì‹¤íŒ¨ ì‹œì—” ë³´ìˆ˜ì ìœ¼ë¡œ True (ì‹¤ì œ ìš”ì•½ì—ì„œ ì˜ë£Œ í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ë ´)\n",
    "            return True\n",
    "\n",
    "    # (E) ê¸°ë³¸ì€ LLM íŒë³„ (ì—„ê²© JSON, ì‹¤íŒ¨ ì‹œ ì°¨ë‹¨)\n",
    "    try:\n",
    "        raw = classifier_chain.invoke({\"user_text\": t_norm})\n",
    "        data = json.loads(raw)\n",
    "        return bool(data.get(\"on_topic\", False))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(\"âœ… ë„ë©”ì¸ ê°€ë“œ(ê°•í™”íŒ) - ì¤€ë¹„ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cce58fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê²€ìƒ‰ì§ˆì˜ ìƒì„±ê¸° - ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# part 3 (1) \n",
    "#==========================================================\n",
    "# ğŸ“Œ ê²€ìƒ‰ì§ˆì˜ ìƒì„±ê¸° (STT â†’ ê´€ë ¨ ê²€ìƒ‰ì–´ 2~4ê°œ)\n",
    "#==========================================================\n",
    "QUERY_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "      (\"system\",\n",
    "       \"ë„ˆëŠ” ì˜ë£Œ ìƒë‹´ ë³´ì¡° ê²€ìƒ‰ì–´ ìƒì„±ê¸°ì´ë‹¤. ì…ë ¥ STTì—ì„œ í•µì‹¬ ì¦ìƒ/í‚¤ì›Œë“œë¥¼ ë½‘ì•„ \"\n",
    "       \"ì¹˜ë§¤/ê²½ë„ì¸ì§€ì¥ì• ì™€ ê´€ë ¨ëœ í•œêµ­ì–´ ê²€ìƒ‰ì§ˆì˜ 2~4ê°œë¥¼ JSON ë°°ì—´ë¡œë§Œ ì¶œë ¥í•˜ë¼.\"),\n",
    "      (\"human\", \"{transcript}\")\n",
    "    ]\n",
    ")\n",
    "query_llm   = ChatOpenAI(model=CHAT_MODEL_NAME, temperature=0.1, max_tokens=120, api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "query_chain = QUERY_PROMPT | query_llm | StrOutputParser()\n",
    "\n",
    "def make_search_queries(transcript: str) -> List[str]:\n",
    "    try:\n",
    "        raw = query_chain.invoke({\"transcript\": transcript})\n",
    "        qs = json.loads(raw)\n",
    "        qs = [q for q in qs if isinstance(q, str)]\n",
    "        base = [\"ì¹˜ë§¤ ì´ˆê¸° ì¦ìƒ\", \"ê²½ë„ì¸ì§€ì¥ì•  ì–¸ì–´ ìœ ì°½ì„±\", \"ì¼ìƒ ì•ˆì „ ê°€ì¡± êµìœ¡\", \"ë‹¨ê¸° ê¸°ì–µë ¥ ì €í•˜ ì›ì¸\"]\n",
    "        return (qs + base)[:4]\n",
    "    except Exception:\n",
    "        return [\"ì¹˜ë§¤ ì´ˆê¸° ì¦ìƒ\", \"ê²½ë„ì¸ì§€ì¥ì•  ì–¸ì–´ ìœ ì°½ì„±\", \"ì¼ìƒ ì•ˆì „ ê°€ì¡± êµìœ¡\", \"ë‹¨ê¸° ê¸°ì–µë ¥ ì €í•˜ ì›ì¸\"]\n",
    "\n",
    "print(\"âœ… ê²€ìƒ‰ì§ˆì˜ ìƒì„±ê¸° - ì¤€ë¹„ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c4feb23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìš”ì•½ ì²´ì¸ & ì €ì¥ í•¨ìˆ˜ - ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# part 3 (2) \n",
    "#==========================================================\n",
    "# ğŸ“Œ ìš”ì•½ ì²´ì¸ (í…œí”Œë¦¿ ê°•ì œ, <ìš”ì•½>ë§Œ) + íŒŒì¼ ì €ì¥ í—¬í¼\n",
    "#==========================================================\n",
    "summary_chain = SUMMARISE_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "def summarise_transcript_only_summary(\n",
    "    transcript: str,\n",
    "    guide_question_index: int = 3,  # 0~3 (Q1~Q4)\n",
    ") -> str:\n",
    "    if not is_on_topic(transcript):\n",
    "        return OFF_TOPIC_MESSAGE\n",
    "\n",
    "    guide_question = GUIDE_QUESTIONS[max(0, min(3, int(guide_question_index)))]\n",
    "    # 1) ì§ˆì˜ ìƒì„±\n",
    "    queries = make_search_queries(transcript)\n",
    "    # 2) Rerank-Then-Summarise RAG\n",
    "    rag_context = build_rag_context_rerank(transcript, queries)\n",
    "\n",
    "    # 3) ìš”ì•½ ìƒì„± (ì˜¤ì§ <ìš”ì•½> ì„¹ì…˜ë§Œ)\n",
    "    out = summary_chain.invoke({\n",
    "        \"transcript\": transcript.strip(),\n",
    "        \"rag_context\": rag_context.strip() if rag_context else \"(ë¬¸ë§¥ ì—†ìŒ)\",\n",
    "        \"guide_question\": guide_question,\n",
    "        \"summary_template\": SUMMARY_TEMPLATE,\n",
    "    })\n",
    "    return out.strip()\n",
    "\n",
    "def summarise_from_txt_and_save(input_path: str, output_path: str, guide_question_index: int = 3, encoding=\"utf-8\") -> str:\n",
    "    with open(input_path, \"r\", encoding=encoding) as f:\n",
    "        text = f.read()\n",
    "    summary_only = summarise_transcript_only_summary(text, guide_question_index=guide_question_index)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(summary_only + \"\\n\")\n",
    "    return output_path\n",
    "\n",
    "print(\"âœ… ìš”ì•½ ì²´ì¸ & ì €ì¥ í•¨ìˆ˜ - ì¤€ë¹„ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f97ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "15a3d6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: ./summary_1_.txt\n",
      "------ ë¯¸ë¦¬ë³´ê¸° ------\n",
      "<ìš”ì•½>\n",
      "1. **ì£¼ ì¦ìƒ**\n",
      "- ë¬¼ê±´ì„ ìƒì–´ë²„ë¦¼\n",
      "- ê¸¸ì„ ìƒì„ê¹Œ ë‘ë ¤ì›€\n",
      "- ì „í™” í†µí™” í›„ ê¸°ì–µ ìƒì‹¤\n",
      "- ë‹¨ì–´ê°€ ì˜ ë– ì˜¤ë¥´ì§€ ì•ŠìŒ\n",
      "\n",
      "2. **ìƒë‹´ë‚´ìš©**\n",
      "- ë¬¼ê±´ì„ ë‘ê³  ì°¾ì§€ ëª»í•´ ê°€ì¡±ì—ê²Œ ìì£¼ ë¬¼ì–´ë´„\n",
      "- í˜¼ì ì™¸ì¶œí•˜ëŠ” ê²ƒì´ ë‘ë ¤ì›€\n",
      "- ì „í™” í†µí™” í›„ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ì§€ ëª»í•¨\n",
      "- ë‹¨ì–´ê°€ ì˜ ë– ì˜¤ë¥´ì§€ ì•Šì•„ ì°½í”¼í•¨\n",
      "\n",
      "3. **ì‹¬ë¦¬ìƒíƒœ**\n",
      "- ë‘ë ¤ì›€\n",
      "  - ê·¼ê±° : â€œí˜¼ì ì™¸ì¶œí•˜ëŠ” ê²ƒë„ ë¬´ì„­ìŠµë‹ˆë‹¤â€\n",
      "- ë¶ˆì•ˆ\n",
      "  - ê·¼ê±° : â€œì œ ìƒíƒœê°€ ì ì  ë” ë‚˜ë¹ ì ¸ì„œ ê°€ì¡±ë“¤ì—ê²Œ ì§ì´ ë ê¹Œ ë´ ê°€ì¥ ê±±ì •ì…ë‹ˆë‹¤â€\n",
      "- ìˆ˜ì¹˜ì‹¬\n",
      "  - ê·¼ê±° : â€œì‚¬ëŒë“¤ì´ ì´ìƒí•˜ê²Œ ìƒê°í• ê¹Œ ë´ ì°½í”¼í•˜ê¸°ë„ í•©ë‹ˆë‹¤â€\n",
      "\n",
      "4. **AI í•´ì„**\n",
      "- í˜„ì¬ ê²ªê³  ê³„ì‹  ì¦ìƒì€ ê²½ë„ì¸ì§€ì¥ì• (MCI)ì™€ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë©°, ì¡°ê¸° ê²€ì§„ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "5. **ì£¼ì˜ì‚¬í•­**\n",
      "- ì¼ìƒì—ì„œ ë¬¼ê±´ì„ ì •í•´ì§„ ì¥ì†Œì— ë‘ëŠ” ìŠµê´€ì„ ê¸°ë¥´ì„¸ìš”.\n",
      "- ì™¸ì¶œ ì‹œ ì•ˆì „í•œ ê²½ë¡œë¥¼ ë¯¸ë¦¬ ê³„íší•˜ê³ , í•„ìš” ì‹œ ë™ë°˜ìë¥¼ ìš”ì²­í•˜ì„¸ìš”.\n",
      "- ì •ì„œì  ì§€ì§€ë¥¼ ìœ„í•´ ê°€ì¡±ê³¼ì˜ ì†Œí†µì„ ê°•í™”í•˜ê³ , ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì—¬ ê²€ì§„ì„ ë°›ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# part 4 (demo)\n",
    "#==========================================================\n",
    "# ğŸ“Œ ë°ëª¨ ì‹¤í–‰: './script_1.txt' â†’ 'summary_1_.txt' ë¡œ ì €ì¥\n",
    "#==========================================================\n",
    "INPUT_TXT  = \"./script_1.txt\"\n",
    "OUTPUT_TXT = \"./summary_1_.txt\"\n",
    "QUESTION_INDEX = 3   # 0:Q1, 1:Q2, 2:Q3, 3:Q4  (ìš”ì²­ëŒ€ë¡œ Q1~Q4 ì¤‘ ì„ íƒ)\n",
    "\n",
    "if not os.path.exists(INPUT_TXT):\n",
    "    print(f\"âš ï¸ {INPUT_TXT} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë°ëª¨ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "    demo_stt = (\n",
    "        \"ìš”ì¦˜ ë¬¼ê±´ì„ ì–´ë””ì— ë‘ì—ˆëŠ”ì§€ ìê¾¸ ìŠì–´ë²„ë ¤ì„œ í˜ë“­ë‹ˆë‹¤. \"\n",
    "        \"ì•ˆê²½ì´ë‚˜ íœ´ëŒ€í°ì„ ë‘ê³ ë„ ì°¾ì§€ë¥¼ ëª»í•´ì„œ ê°€ì¡±ë“¤í•œí…Œ ìì£¼ ë¬¼ì–´ë³´ê²Œ ë¼ìš”. \"\n",
    "        \"ë˜ ë°–ì— ë‚˜ê°”ë‹¤ê°€ ê¸¸ì„ ìƒì„ê¹Œ ë´ í˜¼ì ì™¸ì¶œí•˜ëŠ” ê²ƒë„ ë¬´ì„­ìŠµë‹ˆë‹¤. \"\n",
    "        \"ì§‘ì—ì„œëŠ” ì „í™” í†µí™”ë¥¼ í•˜ê³  ë‚˜ë©´ ê¸ˆë°© ë¬´ìŠ¨ ì´ì•¼ê¸°ë¥¼ í–ˆëŠ”ì§€ ê¸°ì–µì´ ì•ˆ ë‚˜ì„œ ë‹µë‹µí•˜ê³ ìš”. \"\n",
    "        \"ì œ ìƒíƒœê°€ ì ì  ë” ë‚˜ë¹ ì ¸ì„œ ê°€ì¡±ë“¤ì—ê²Œ ì§ì´ ë ê¹Œ ë´ ê°€ì¥ ê±±ì •ì…ë‹ˆë‹¤. \"\n",
    "        \"ëŒ€í™”í•  ë•Œ ë‹¨ì–´ê°€ ì˜ ë– ì˜¤ë¥´ì§€ ì•Šì•„ì„œ ì‚¬ëŒë“¤ì´ ì´ìƒí•˜ê²Œ ìƒê°í• ê¹Œ ë´ ì°½í”¼í•˜ê¸°ë„ í•©ë‹ˆë‹¤.\"\n",
    "    )\n",
    "    with open(INPUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(demo_stt)\n",
    "\n",
    "out_path = summarise_from_txt_and_save(INPUT_TXT, OUTPUT_TXT, guide_question_index=QUESTION_INDEX)\n",
    "print(f\"âœ… ì €ì¥ ì™„ë£Œ: {out_path}\")\n",
    "with open(out_path, 'r', encoding='utf-8') as f:\n",
    "    preview = f.read()\n",
    "print('------ ë¯¸ë¦¬ë³´ê¸° ------')\n",
    "print(preview[:1200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3099cade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… (ì„ íƒ) CLI ë£¨í”„ ì¤€ë¹„ë¨ â€” í•„ìš” ì‹œ start_chat() ì‹¤í–‰\n"
     ]
    }
   ],
   "source": [
    "# part 5 (optional cli)\n",
    "#==========================================================\n",
    "# ğŸ“Œ ê°„ë‹¨ ëŒ€í™” ë£¨í”„ (ì˜¤í”„í† í”½ ì°¨ë‹¨ + ìš”ì•½)\n",
    "#==========================================================\n",
    "# ì‚¬ìš©ë²•:\n",
    "#  - ì²« ë°œí™”ëŠ” 4ê°œ ê³ ì • ì§ˆë¬¸ ì¤‘ í•˜ë‚˜ë¥¼ ì¶œë ¥\n",
    "#  - ì´í›„ ì‚¬ìš©ìì˜ ê¸´ ì‘ë‹µ(í…ìŠ¤íŠ¸)ì„ ë°›ì•„ ìš”ì•½ í…œí”Œë¦¿ìœ¼ë¡œ ì •ë¦¬\n",
    "\n",
    "def start_chat(guide_question_index: int = 3):\n",
    "    q = GUIDE_QUESTIONS[max(0, min(3, int(guide_question_index)))]\n",
    "    print(f\"[ìƒë‹´ ì±—ë´‡] {q}\")\n",
    "    print(\"[ì•ˆë‚´] ê¸´ ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš”. '/quit' ì…ë ¥ ì‹œ ì¢…ë£Œ.\")\n",
    "    while True:\n",
    "        user = input(\"\\n[ì‚¬ìš©ì] \").strip()\n",
    "        if user.lower() in {\"/quit\", \"quit\", \"exit\"}:\n",
    "            print(\"ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "            break\n",
    "        result = summarise_transcript(user, guide_question_index=guide_question_index)\n",
    "        print(\"\\n[ìš”ì•½]\\n\")\n",
    "        print(result)\n",
    "\n",
    "# start_chat(guide_question_index=3)  # â† í•„ìš” ì‹œ ì£¼ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰\n",
    "print(\"âœ… (ì„ íƒ) CLI ë£¨í”„ ì¤€ë¹„ë¨ â€” í•„ìš” ì‹œ start_chat() ì‹¤í–‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fbcd085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¹˜ë§¤ê´€ë ¨ ìƒë‹´ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# part 6\n",
    "#==========================================================\n",
    "# ğŸ“Œ ê°„ë‹¨ ëŒ€í™” ë£¨í”„ (ì˜¤í”„í† í”½ ì°¨ë‹¨ + ìš”ì•½)\n",
    "#==========================================================\n",
    "print(summarise_from_txt(\"./script_2.txt\", guide_question_index=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d63328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
