# -*- coding: utf-8 -*-
"""
chabot_model.py
- LLM + LangChain + RAG íŒŒì´í”„ë¼ì¸
- ì˜¨í† í”½ ê°ì§€(LLM), ê°ì •/ê·¼ê±°/í‚¤ì›Œë“œ ì¶”ì¶œ(LLM), ìš”ì•½(<ìš”ì•½> ì„¹ì…˜), êµ¬ì¡°ì  JSON ìš”ì•½
- âœ… í—ˆìš© ëª¨ë¸ì„ 'gpt-4o'ì™€ 'gpt-4o-mini'ë¡œ **ì—„ê²© ì œí•œ**
- âœ… ê¸°ë³¸ì€ gpt-4o, ì‚¬ìš©ìê°€ ì›í•˜ë©´ gpt-4o-mini ì„ íƒ ê°€ëŠ¥
- âœ… ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ëª… ì…ë ¥ ì‹œ ValueError (ì„œë²„ì—ì„œ 400ìœ¼ë¡œ ë‚´ë ¤ì£¼ê¸¸ ê¶Œì¥)
"""

import os, json, re, time
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document


# --- PATCH: loose JSON parser & helpers ---
import re as _re

def _json_loose_loads(text: str) -> dict:
    """
    ëª¨ë¸ì´ JSONì„ ì½”ë“œíœìŠ¤ë¡œ ê°ì‹¸ê±°ë‚˜ ìŠ¤ë§ˆíŠ¸ë”°ì˜´í‘œ/íŠ¸ë ˆì¼ë§ ì½¤ë§ˆê°€ ì„ì—¬ë„ ìµœëŒ€í•œ íŒŒì‹±.
    ì‹¤íŒ¨ ì‹œ ValueErrorë¥¼ ê·¸ëŒ€ë¡œ ë˜ì§.
    """
    if not text:
        raise ValueError("empty")
    # ì½”ë“œíœìŠ¤ ì œê±°
    t = text.strip()
    t = _re.sub(r"^```(?:json)?\s*", "", t)
    t = _re.sub(r"\s*```$", "", t)

    # ì²« ë²ˆì§¸ { ... } ë¸”ë¡ë§Œ ì¶”ì¶œ
    m = _re.search(r"\{.*\}", t, flags=_re.DOTALL)
    if not m:
        raise ValueError("no-json-object")
    s = m.group(0)

    # ìŠ¤ë§ˆíŠ¸ ë”°ì˜´í‘œ ì •ê·œí™”
    s = s.replace("â€œ", "\"").replace("â€", "\"").replace("â€™", "'").replace("â€˜", "'")

    # }ë‚˜ ] ì•ì˜ íŠ¸ë ˆì¼ë§ ì½¤ë§ˆ ì œê±°
    s = _re.sub(r",(\s*[}\]])", r"\1", s)

    return json.loads(s)

def _first_sentence_containing(text: str, keyword: str) -> str:
    for sent in _re.split(r"[.!?\n]+", text):
        if keyword in sent:
            st = sent.strip()
            if 6 <= len(st) <= 120:
                return st
    return ""

# -------------------------------
# 0) í™˜ê²½/í‚¤ ë¡œë”©
# -------------------------------
load_dotenv()

API_TOKEN_PATH = os.getenv("API_TOKEN", "./api_token.txt")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
DEV_USE_TOKEN_FILE = os.getenv("DEV_USE_TOKEN_FILE", "1") == "1"

if not OPENAI_API_KEY and DEV_USE_TOKEN_FILE and os.path.exists(API_TOKEN_PATH):
    try:
        with open(API_TOKEN_PATH, "r", encoding="utf-8") as f:
            k = f.read().strip()
        if k:
            os.environ["OPENAI_API_KEY"] = k
            OPENAI_API_KEY = k
            print("âœ… OpenAI API í‚¤ë¥¼ api_token.txtì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ API í† í° ë¡œë“œ ì‹¤íŒ¨: {e}")

if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# -------------------------------
# 1) ê¸°ë³¸ê°’ (ìš”ì²­ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
# -------------------------------
# âœ… ê¸°ë³¸ì„ gpt-4oë¡œ ê³ ì •
DEFAULT_CHAT_MODEL  = os.getenv("OPENAI_CHAT_MODEL", "gpt-4o")
DEFAULT_EMBED_MODEL = os.getenv("OPENAI_EMBED_MODEL", "text-embedding-3-small")
DEFAULT_TEMPERATURE = float(os.getenv("TEMPERATURE", "0.2"))
DEFAULT_MAX_TOKENS  = int(os.getenv("MAX_OUTPUT_TOKENS", "800"))

OFF_TOPIC_MESSAGE = "ì¹˜ë§¤ì™€ ê´€ë ¨ì—†ëŠ” ë‚´ìš©ìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤. í•´ë‹¹ ì±—ë´‡ì€ ì¹˜ë§¤ì™€ ì¸ì§€ì¥ì•  ê´€ë ¨ ìƒë‹´ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."

# ê³ ì • ì§ˆë¬¸
Q1 = 'ìì£¼ ì“°ë˜ ë¬¼ê±´ ì´ë¦„ì´ ê°‘ìê¸° ìƒê°ì•ˆ ë‚œì ì´ ìˆë‚˜ìš”?'
Q2 = 'ëŒ€í™”ì¤‘ë‹¨ì–´ê°€ ì˜ ë– ì˜¤ë¥´ì§€ ì•Šì•„ì„œ ê³¤ë€í–ˆë˜ ì ì´ ìˆë‚˜ìš”?'
Q3 = 'ê°€ì¡±ì´ë‚˜ ì§€ì¸ì´ í‰ì†Œì™€ ë‹¤ë¥´ë‹¤ê³  í•œì ì´ ìˆë‚˜ìš”?'
Q4 = 'ìµœê·¼ì— ë¶ˆí¸í–ˆë˜ ì ì´ë‚˜ ê±±ì •ë˜ëŠ” ì ì´ ìˆë‚˜ìš”?'
GUIDE_QUESTIONS = [Q1, Q2, Q3, Q4]

# -------------------------------
# 2) ëª¨ë¸/í´ë¼ì´ì–¸íŠ¸
# -------------------------------
@dataclass
class ClientBundle:
    llm_summary: ChatOpenAI
    llm_judge:   ChatOpenAI
    llm_query:   ChatOpenAI
    llm_emo:     ChatOpenAI
    embeddings:  OpenAIEmbeddings
    temperature: float
    max_tokens:  int
    model_ids:   Dict[str, str]

# âœ… í—ˆìš© ëª¨ë¸ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ (ì±—/ì„ë² ë”©)
ALLOWED_CHAT = {"gpt-4o", "gpt-4o-mini"}
ALLOWED_EMBED = {"text-embedding-3-small", "text-embedding-3-large"}

def _normalise_model_id(mid: Optional[str], kind: str) -> Optional[str]:
    """
    kind: "chat" | "embed"
    - í”í•œ ë³„ì¹­/ì˜¤íƒ€ë§Œ gpt-4o / gpt-4o-minië¡œ ë³´ì •
    - ê·¸ ì™¸ëŠ” ValueError (ì—„ê²©)
    """
    if not mid:
        return None
    m = mid.strip()

    # ë³„ì¹­ ë³´ì • (ë”± 4o ê³„ì—´ë§Œ í—ˆìš©)
    aliases = {
        "gpt4o": "gpt-4o",
        "gpt-o4": "gpt-4o",
        "gpt-4-omni": "gpt-4o",
        "4o": "gpt-4o",
        "4o-mini": "gpt-4o-mini",
        # 'o4', 'o4-mini' ë“±ì€ í—ˆìš©í•˜ì§€ ì•ŠìŒ
        "text-embedding-3small": "text-embedding-3-small",
        "text-embedding-3large": "text-embedding-3-large",
    }
    m = aliases.get(m, m)

    if kind == "chat":
        if m not in ALLOWED_CHAT:
            raise ValueError(f"Unsupported chat model: {mid}. Allowed: {sorted(ALLOWED_CHAT)}")
    else:
        if m not in ALLOWED_EMBED:
            raise ValueError(f"Unsupported embedding model: {mid}. Allowed: {sorted(ALLOWED_EMBED)}")
    return m

def _build_clients(
    chat_model: Optional[str] = None,
    embed_model: Optional[str] = None,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    models: Optional[Dict[str, str]] = None,
) -> ClientBundle:
    # âœ… ì—„ê²© ì •ê·œí™”
    cm_in  = _normalise_model_id(chat_model  or DEFAULT_CHAT_MODEL,  "chat")
    em_in  = _normalise_model_id(embed_model or DEFAULT_EMBED_MODEL, "embed")

    tmp = DEFAULT_TEMPERATURE if temperature is None else float(temperature)
    mx  = DEFAULT_MAX_TOKENS  if max_tokens  is None else int(max_tokens)

    m_summary = m_judge = m_query = m_emo = cm_in
    m_embed   = em_in

    if models and isinstance(models, dict):
        # êµ¬ì„±ìš”ì†Œë³„ ì§€ì •ë„ í—ˆìš© (ë‹¨, í—ˆìš© ëª¨ë¸ ë‚´)
        if "summary" in models: m_summary = _normalise_model_id(models["summary"], "chat")
        if "judge"   in models: m_judge   = _normalise_model_id(models["judge"],   "chat")
        if "query"   in models: m_query   = _normalise_model_id(models["query"],   "chat")
        if "emotion" in models: m_emo     = _normalise_model_id(models["emotion"], "chat")
        if "embed"   in models: m_embed   = _normalise_model_id(models["embed"],   "embed")

    # ì¸ìŠ¤í„´ìŠ¤
    llm_summary = ChatOpenAI(model=m_summary, temperature=tmp, max_tokens=mx, api_key=OPENAI_API_KEY)
    llm_judge   = ChatOpenAI(model=m_judge,   temperature=0.0, max_tokens=min(mx, 220), api_key=OPENAI_API_KEY)
    llm_query   = ChatOpenAI(model=m_query,   temperature=0.1, max_tokens=min(mx, 120), api_key=OPENAI_API_KEY)
    llm_emo     = ChatOpenAI(model=m_emo,     temperature=0.1, max_tokens=min(mx, 500), api_key=OPENAI_API_KEY)
    embeddings  = OpenAIEmbeddings(model=m_embed, api_key=OPENAI_API_KEY)

    return ClientBundle(
        llm_summary=llm_summary,
        llm_judge=llm_judge,
        llm_query=llm_query,
        llm_emo=llm_emo,
        embeddings=embeddings,
        temperature=tmp,
        max_tokens=mx,
        model_ids={
            "summary": m_summary,
            "judge":   m_judge,
            "emotion": m_emo,
            "query":   m_query,
            "embed":   m_embed,
        }
    )

# -------------------------------
# 3) ì˜¨í† í”½ ê°ì§€(LLM) â€” ì „ì²´ ë¬¸ì¥
# -------------------------------
CLASSIFY_PROMPT = ChatPromptTemplate.from_messages([
    ("system",
     "ë„ˆëŠ” ì…ë ¥ì´ â€˜ì¹˜ë§¤/ì¸ì§€ì¥ì•  ìƒë‹´â€™ì¸ì§€ íŒë‹¨í•˜ëŠ” ë¶„ë¥˜ê¸°ë‹¤. "
     "íŒë‹¨ ê¸°ì¤€: (a) ê¸°ì–µë ¥ ì €í•˜/ë‹¨ì–´ íšŒìƒ ê³¤ë€/ì–¸ì–´ ìœ ì°½ì„± ì €í•˜/ë°©í–¥ê°ê° ë¬¸ì œ/ì¼ìƒìƒí™œ ê³¤ë€/ì •ì„œë°˜ì‘ ë“± êµ¬ì²´ì  ì„œìˆ , "
     "(b) ë³´í˜¸ì ê´€ì°°/ê²€ì‚¬/í‰ê°€/ì•ˆì „ ë¬¸ì œ/ê°€ì¡± êµìœ¡. "
     "ë©”íƒ€ ëŒ€í™”(ë„ˆëŠ” ëˆ„êµ¬ëƒ, ìƒë‹´ ë˜ëƒ ë“±), ì¼ë°˜ ì¡ë‹´, ë¹„ì¹˜ë§¤ ì¼ë°˜ ê±´ê°•/ì¼ìƒì€ off_topic. "
     "ì˜¤ì§ JSONë§Œ ì¶œë ¥: "
     "{{\"on_topic\": true|false, \"score\": 0.0~1.0, "
     "\"evidence_spans\": [\"...\", \"...\"], \"reason\": \"...\"}}. "
     "evidence_spansëŠ” ì›ë¬¸ ë°œì·Œ 1~3ê°œ(ê° 6~40ì)."),
    ("human", "ì…ë ¥:\n\"\"\"\n{user_text}\n\"\"\"\nì˜¤ì§ JSON:")
])

DEFAULT_PRIOR = 0.60
SESSION_STATE: Dict[str, Dict] = {}

def _get_session(session_id: Optional[str]) -> Dict:
    sid = session_id or "default"
    if sid not in SESSION_STATE:
        SESSION_STATE[sid] = {"prior": DEFAULT_PRIOR, "last_ts": time.time(), "history": []}
    return SESSION_STATE[sid]

def mark_guide_question_shown(session_id: Optional[str]):
    s = _get_session(session_id)
    s["prior"] = max(s["prior"], 0.75)

def _update_session(session_id: Optional[str], label: str, prob: float):
    s = _get_session(session_id)
    s["history"].append({"label": label, "prob": prob})
    s["last_ts"] = time.time()
    last = s["history"][-3:]
    s["prior"] = 0.5 * s["prior"] + 0.5 * (sum(h["prob"] for h in last) / len(last))

TAU_ON = 0.60
BAND   = (0.48, 0.60)

def detect_topic(text: str, judge_llm: ChatOpenAI, session_id: Optional[str] = None) -> Dict[str, Any]:
    parser = StrOutputParser()
    judge_chain = CLASSIFY_PROMPT | judge_llm | parser
    t = (text or "").strip()
    if not t:
        return {"label": "off_topic", "prob": 0.0, "evidence": []}
    
    try:
        raw = judge_chain.invoke({"user_text": t})
        print(f"ğŸ” LLM ì›ë³¸ ì‘ë‹µ: {raw}")  # ë””ë²„ê¹…ìš©
        
        # ì½”ë“œíœìŠ¤ ì œê±° (```json, ``` ë“±)
        cleaned_raw = raw.strip()
        if cleaned_raw.startswith("```"):
            # ì²« ë²ˆì§¸ ``` ì œê±°
            cleaned_raw = cleaned_raw[3:]
            # ë§ˆì§€ë§‰ ``` ì œê±°
            if cleaned_raw.endswith("```"):
                cleaned_raw = cleaned_raw[:-3]
            # json, python ë“± ì–¸ì–´ í‘œì‹œ ì œê±°
            cleaned_raw = cleaned_raw.lstrip("json\n").lstrip("python\n").lstrip("```\n")
        
        cleaned_raw = cleaned_raw.strip()
        print(f"ğŸ§¹ ì •ë¦¬ëœ ì‘ë‹µ: {cleaned_raw}")  # ë””ë²„ê¹…ìš©
        
        data = json.loads(cleaned_raw)
        on_topic = bool(data.get("on_topic", False))
        score = float(data.get("score", 0.0))
        reason = str(data.get("reason", ""))
        
        # on_topicì´ falseë©´ í™•ì‹¤íˆ off_topic
        if not on_topic:
            return {"label": "off_topic", "prob": score, "evidence": [reason]}
        
        # on_topicì´ trueì´ê³  scoreê°€ ë†’ìœ¼ë©´ on_topic
        if score >= TAU_ON:
            return {"label": "on_topic", "prob": score, "evidence": [reason]}
        else:
            return {"label": "off_topic", "prob": score, "evidence": [reason]}
            
    except Exception as e:
        print(f"âš ï¸ detect_topic ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ off_topic
        return {"label": "off_topic", "prob": 0.0, "evidence": ["íŒŒì‹± ì˜¤ë¥˜"]}

# -------------------------------
# 4) ì„¸ê·¸ë¨¼íŠ¸ ë‹¨ìœ„ íŒë³„ + ë¹„ì¹˜ë§¤ ê³¼ì—… ê°ì§€
# -------------------------------
SEGMENT_CLASSIFY_PROMPT = ChatPromptTemplate.from_messages([
    ("system",
     "ë„ˆëŠ” ë¬¸ì¥ì´ â€˜ì¹˜ë§¤/ì¸ì§€ì¥ì•  ìƒë‹´â€™ì— í•´ë‹¹í•˜ëŠ”ì§€ íŒë‹¨í•˜ëŠ” ë¶„ë¥˜ê¸°ë‹¤.\n"
     "íŒë‹¨ ê¸°ì¤€: (a) ê¸°ì–µì €í•˜/ë‹¨ì–´íšŒìƒê³¤ë€/ì–¸ì–´ìœ ì°½ì„± ì €í•˜/ê¸¸ìƒìŒ/ë°©í–¥ê°ê° ë¬¸ì œ/ì¼ìƒìƒí™œ ê³¤ë€/ì •ì„œë°˜ì‘(ë¶ˆì•ˆÂ·ë‘ë ¤ì›€Â·ìˆ˜ì¹˜ì‹¬ ë“±) ì¤‘ "
     "ì ì–´ë„ í•˜ë‚˜ì˜ **êµ¬ì²´ì  ê²½í—˜/ìš°ë ¤**ê°€ ë‚˜íƒ€ë‚˜ë©´ on_topic. ì¼ìƒ ì¶”ì²œ/ë©”ë‰´ ì„ íƒ/ê´‘ê³ /ê°€ê²©/ì¼ë°˜ ìˆ˜ë‹¤ëŠ” off_topic.\n"
     "ì˜¤ì§ JSON: {{\"on_topic\": true|false, \"score\": 0.0~1.0}}"),
    ("human", "ë¬¸ì¥:\n\"\"\"\nìš”ì¦˜ ì•½ì† ì¥ì†Œ ì´ë¦„ì´ ìê¾¸ ìƒê°ì´ ì•ˆ ë‚˜ìš”. ë©‹ì©ì–´ì„œ ì›ƒê³  ë„˜ì–´ê°€ìš”.\n\"\"\"\nì˜¤ì§ JSON:"),
    ("assistant", "{\"on_topic\": true, \"score\": 0.86}"),
    ("human", "ë¬¸ì¥:\n\"\"\"\nì˜¤ëŠ˜ ì™€í¼ ë¨¹ì„ê¹Œ í†µìƒˆìš° ì™€í¼ ë¨¹ì„ê¹Œ ê³¨ë¼ì¤˜.\n\"\"\"\nì˜¤ì§ JSON:"),
    ("assistant", "{\"on_topic\": false, \"score\": 0.05}"),
    ("human", "ë¬¸ì¥:\n\"\"\"\nëŒ€í™”í•˜ë‹¤ê°€ ë‹¨ì–´ê°€ ìê¾¸ ë§‰í˜€ìš”. ê·¸ê²Œ ë– ì˜¤ë¥´ì§€ ì•Šì•„ì„œ ëŒ€í™” íë¦„ì´ ëŠê²¨ìš”.\n\"\"\"\nì˜¤ì§ JSON:"),
    ("assistant", "{\"on_topic\": true, \"score\": 0.88}"),
    ("human", "ë¬¸ì¥:\n\"\"\"\nê¸°ì–µì´ ì˜ ì•ˆ ë‚˜ì„œ ë¬´ì„­ê³  ë‹¹í™©ìŠ¤ëŸ¬ì›Œìš”.\n\"\"\"\nì˜¤ì§ JSON:"),
    ("assistant", "{\"on_topic\": true, \"score\": 0.9}"),
    ("human", "ë¬¸ì¥:\n\"\"\"\nê·¼ë° ì¹˜í‚¨ì´ë‘ í”¼ì ì¤‘ì— ë­ê°€ ë” ì¢‹ì•„?\n\"\"\"\nì˜¤ì§ JSON:"),
    ("assistant", "{\"on_topic\": false, \"score\": 0.07}"),
    ("human", "ë¬¸ì¥:\n\"\"\"\n{sent}\n\"\"\"\nì˜¤ì§ JSON:")
])
_segment_judge_parser = StrOutputParser()

OFFDOMAIN_TASK_PROMPT = ChatPromptTemplate.from_messages([
    ("system",
     "ì…ë ¥ì´ â€˜ì¹˜ë§¤ìƒë‹´â€™ ì´ì™¸ì˜ **ì‹¤ì œ ê³¼ì—… ìš”ì²­**(ì˜ˆ: ë©”ë‰´ ì¶”ì²œ, ì‹ë‹¹/ìŒì‹/ì‡¼í•‘/ê°€ê²©/ì˜ì–‘/ì¿ í°/ê´‘ê³ /ê²Œì„/ì½”ë”©/ë‚ ì”¨ ë“±)ì„ í¬í•¨í•˜ëŠ”ì§€ íŒì •í•˜ë¼.\n"
     "ì˜¤ì§ JSON: {{\"non_dementia_task\": true|false, \"spans\": [\"...\", \"...\"]}}"),
    ("human", "ì…ë ¥:\n\"\"\"\n{whole}\n\"\"\"\nì˜¤ì§ JSON:")
])
_offdomain_parser = StrOutputParser()

def _split_into_segments(text: str) -> List[str]:
    raw = re.split(r"(?:\n+|[.!?]+|\r+|^\s*\d+[.)]\s*)", text)
    segs = [s.strip() for s in raw if s and s.strip()]
    return [s for s in segs if len(s) >= 2]

def classify_segments(text: str, judge_llm: ChatOpenAI) -> List[Dict[str, Any]]:
    segments = _split_into_segments(text)
    chain = SEGMENT_CLASSIFY_PROMPT | judge_llm | _segment_judge_parser
    out: List[Dict[str, Any]] = []
    for s in segments:
        try:
            js = json.loads(chain.invoke({"sent": s}))
            out.append({"text": s, "on_topic": bool(js.get("on_topic", False)), "score": float(js.get("score", 0.5))})
        except Exception:
            out.append({"text": s, "on_topic": False, "score": 0.5})
    # í´ë°±(ë™ì¼ ëª¨ë¸ì—ì„œ ëª¨ë‘ 0.5 default ëŠë‚Œì¼ ë•Œ, minië¡œ ì¬ì‹œë„)
    if out and all((not x["on_topic"] and abs(x["score"] - 0.5) < 1e-9) for x in out):
        try:
            fb_judge = ChatOpenAI(model="gpt-4o-mini", temperature=0.0, max_tokens=220, api_key=OPENAI_API_KEY)
            fb_chain = SEGMENT_CLASSIFY_PROMPT | fb_judge | _segment_judge_parser
            out_fb: List[Dict[str, Any]] = []
            for s in segments:
                try:
                    js = json.loads(fb_chain.invoke({"sent": s}))
                    out_fb.append({"text": s, "on_topic": bool(js.get("on_topic", False)), "score": float(js.get("score", 0.5))})
                except Exception:
                    out_fb.append({"text": s, "on_topic": False, "score": 0.5})
            if any(x["on_topic"] or abs(x["score"] - 0.5) > 0.0 for x in out_fb):
                out = out_fb
        except Exception:
            pass
    return out

def detect_offdomain_task(text: str, judge_llm: ChatOpenAI) -> Dict[str, Any]:
    chain = OFFDOMAIN_TASK_PROMPT | judge_llm | _offdomain_parser
    try:
        js = json.loads(chain.invoke({"whole": text}))
        return {
            "non_dementia_task": bool(js.get("non_dementia_task", False)),
            "spans": [str(x) for x in (js.get("spans") or [])][:2]
        }
    except Exception:
        return {"non_dementia_task": False, "spans": []}

RESCUE_TRIGGERS = re.compile(
    r"(ê¸°ì–µ|ë– ì˜¬ë¦¬|ìƒê°ì´\s*ì•ˆ\s*ë‚˜|ë‹¨ì–´|ë§ì´\s*ë§‰íˆ|ê¹Œë¨¹|ë¬´ì„œì›Œ|ë‘ë µ|ë©‹ì©|ë‹¹í™©)",
    re.IGNORECASE
)

def apply_topic_policy(text: str, judge_llm: ChatOpenAI) -> Dict[str, Any]:
    segs = classify_segments(text, judge_llm)
    if not segs:
        return {"label": "off_topic", "coverage": 0.0, "filtered_text": "", "dropped_segments": [], "offdomain": {}, "segments": []}

    on = [s for s in segs if s["on_topic"]]
    coverage = len(on) / max(1, len(segs))
    offd = detect_offdomain_task(text, judge_llm)

    if len(on) == 0 and RESCUE_TRIGGERS.search(text):
        on = segs
        coverage = 1.0

    STRONG_ON_EXISTS = any(s["score"] >= 0.70 for s in on)
    if coverage < 0.25 and not STRONG_ON_EXISTS:
        label = "off_topic"
    elif 0.25 <= coverage < 0.55 or offd["non_dementia_task"]:
        label = "partial"
    else:
        label = "on_topic"

    filtered = " ".join([s["text"] for s in on]).strip()
    dropped = [s["text"] for s in segs if s not in on]

    return {
        "label": label,
        "coverage": coverage,
        "filtered_text": filtered,
        "dropped_segments": dropped,
        "offdomain": offd,
        "segments": segs
    }

# -------------------------------
# 5) ê²€ìƒ‰ â†’ RAG ì»¨í…ìŠ¤íŠ¸
# -------------------------------
QUERY_PROMPT = ChatPromptTemplate.from_messages([
    ("system",
     "ì˜ë£Œ ìƒë‹´ ë³´ì¡° ê²€ìƒ‰ì–´ ìƒì„±ê¸°ì´ë‹¤. ì…ë ¥ STTì—ì„œ í•µì‹¬ ì¦ìƒ/í‚¤ì›Œë“œë¥¼ ë½‘ì•„ "
     "ì¹˜ë§¤/ê²½ë„ì¸ì§€ì¥ì• ì™€ ê´€ë ¨ëœ í•œêµ­ì–´ ê²€ìƒ‰ì§ˆì˜ 2~4ê°œë¥¼ JSON ë°°ì—´ë¡œë§Œ ì¶œë ¥í•˜ë¼."),
    ("human", "{transcript}")
])

def make_search_queries(transcript: str, query_llm: ChatOpenAI) -> List[str]:
    parser = StrOutputParser()
    chain = QUERY_PROMPT | query_llm | parser
    base = ["ì¹˜ë§¤ ì´ˆê¸° ì¦ìƒ", "ê²½ë„ì¸ì§€ì¥ì•  ì–¸ì–´ ìœ ì°½ì„±", "ì¼ìƒ ì•ˆì „ ê°€ì¡± êµìœ¡", "ë‹¨ê¸° ê¸°ì–µë ¥ ì €í•˜ ì›ì¸"]
    try:
        raw = chain.invoke({"transcript": transcript})
        qs = [q for q in json.loads(raw) if isinstance(q, str)]
        return (qs + base)[:4]
    except Exception:
        return base[:4]

def ddgs_search(query: str, k: int = 5) -> List[Document]:
    try:
        from ddgs import DDGS
    except Exception:
        return []
    docs: List[Document] = []
    try:
        with DDGS() as d:
            for i, item in enumerate(d.text(query, max_results=k)):
                title = item.get("title") or ""
                link  = item.get("href") or ""
                body  = item.get("body") or ""
                docs.append(Document(
                    page_content=f"{title}\n{body}\nURL: {link}",
                    metadata={"source": link, "title": title, "engine": "ddgs"}))
                if i+1 >= k:
                    break
    except Exception as e:
        print(f"âš ï¸ ddgs ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    return docs

def multi_engine_search(query: str, k: int = 5) -> List[Document]:
    return ddgs_search(query, k)

def _embed_texts(texts: List[str], embeddings: OpenAIEmbeddings) -> List[List[float]]:
    return embeddings.embed_documents(texts)

def _embed_query(text: str, embeddings: OpenAIEmbeddings) -> List[float]:
    return embeddings.embed_query(text)

def _cosine(a: List[float], b: List[float]) -> float:
    import numpy as np
    a = np.array(a, dtype=float); b = np.array(b, dtype=float)
    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + 1e-12
    return float((a @ b) / denom)

def build_rag_context_rerank(
    transcript: str,
    queries: List[str],
    embeddings: OpenAIEmbeddings,
    k_search: int = 8,
    k_rerank: int = 8,
    k_final: int  = 4
) -> Tuple[str, List[Dict[str, str]]]:
    all_docs: List[Document] = []
    for q in queries:
        all_docs.extend(multi_engine_search(q, k=max(3, k_search // max(1, len(queries)) + 1)))
    if not all_docs:
        return "", []
    combined_query = ((" ".join(queries)) + " " + transcript[:600]).strip()
    q_emb = _embed_query(combined_query, embeddings)
    cand = all_docs[:k_rerank]
    cand_texts = [d.page_content for d in cand]
    cand_embs  = _embed_texts(cand_texts, embeddings)
    scored = [(_cosine(q_emb, e), i) for i, e in enumerate(cand_embs)]
    scored.sort(key=lambda x: x[0], reverse=True)
    top_docs = [cand[i] for (_, i) in scored[:k_final]]
    rag_text = "\n\n".join([d.page_content[:1200] for d in top_docs])
    sources = [{"title": d.metadata.get("title",""), "url": d.metadata.get("source",""), "engine": d.metadata.get("engine","")} for d in top_docs]
    return rag_text, sources

# -------------------------------
# 6) ê°ì •/ê·¼ê±°/í‚¤ì›Œë“œ ì¶”ì¶œ(LLM)
# -------------------------------
EMO_PROMPT = ChatPromptTemplate.from_messages([
    ("system",
     "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê°ì • ìƒíƒœë¥¼ ì‹ë³„í•˜ë¼. ê°ì • ë¼ë²¨ì€ ì…ë ¥ìœ¼ë¡œë¶€í„° ìì—°ìŠ¤ëŸ½ê²Œ ë„ì¶œëœ í•œêµ­ì–´ ëª…ì‚¬í˜•(ì˜ˆ: ë‘ë ¤ì›€, ë¶ˆì•ˆê°, ìˆ˜ì¹˜ì‹¬, ë‹¹í™©ìŠ¤ëŸ¬ì›€, ë‹µë‹µí•¨ ë“±)ìœ¼ë¡œ ì‘ì„±. "
     "ê° ê°ì • í•­ëª©:\n"
     " - evidence_sentences: ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°œì·Œ 1~2ê°œ(ê° 6~100ì)\n"
     " - keywords: ì›ë¬¸/ê·¼ê±°ë¬¸ì¥ì— ì‹¤ì œë¡œ ë“±ì¥í•˜ê±°ë‚˜ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´/êµ¬ 3~8ê°œ\n"
     "ìœ ì‚¬ ê°ì • í†µí•©, ì¤‘ë³µ ë¬¸ì¥ ì œê±°, ì´ 1~4ê°œ. "
     "ì˜¤ì§ JSON ë°°ì—´ë§Œ ì¶œë ¥: "
     "[{{\"emotion\":\"...\", \"evidence_sentences\":[\"...\"], \"keywords\":[\"...\"]}}, ...]"),
    ("human", "í…ìŠ¤íŠ¸:\n{transcript}\nì˜¤ì§ JSON:")
])

FORCE_PROMPT = ChatPromptTemplate.from_messages([
    ("system", "ìµœì†Œ 1ê°œ ê°ì • í•­ëª©ì„ ë°˜ë“œì‹œ ìƒì„±í•œë‹¤. í¬ë§·/ì œì•½ ë™ì¼. ì˜¤ì§ JSON. "
               "ì˜ˆì‹œ í˜•ì‹: "
               "[{{\"emotion\":\"...\", \"evidence_sentences\":[\"...\"], \"keywords\":[\"...\"]}}]"),
    ("human", "í…ìŠ¤íŠ¸:\n{transcript}\nì˜¤ì§ JSON:")
])

def extract_emotions_with_keywords(transcript: str, emo_llm: ChatOpenAI) -> List[Dict]:
    parser = StrOutputParser()
    emo_chain   = EMO_PROMPT   | emo_llm | parser
    force_chain = FORCE_PROMPT | emo_llm | parser

    def _top_keywords_from_text(text: str, k: int=6) -> List[str]:
        toks = re.findall(r"[ê°€-í£]{2,}", text)
        out, seen = [], set()
        for t in toks:
            if t not in seen:
                seen.add(t); out.append(t)
            if len(out) >= k:
                break
        return out

    def _parse_emo_json(raw: str) -> List[Dict]:
        def _to_json(r: str) -> Any:
            try:
                return json.loads(r)
            except Exception:
                # ì½”ë“œíœìŠ¤/ìŠ¤ë§ˆíŠ¸ë”°ì˜´í‘œ/íŠ¸ë ˆì¼ë§ ì½¤ë§ˆ ë“± ë³´ì •
                return _json_loose_loads(r)
    
        try:
            data = _to_json(raw)
            if not isinstance(data, list):
                return []
            items = []
            for it in data:
                emo = (it.get("emotion") or "").strip()
                evs = [e.strip() for e in (it.get("evidence_sentences") or []) if isinstance(e, str)]
                kws = [k.strip() for k in (it.get("keywords") or []) if isinstance(k, str)]
                evs = [e for e in evs if 6 <= len(e) <= 120][:2]
                if not emo or not evs:
                    continue
                if len(kws) < 3:
                    for e in evs:
                        # ê°„ë‹¨ í‚¤ì›Œë“œ ë³´ê°•
                        toks = _re.findall(r"[ê°€-í£]{2,}", e)
                        for t in toks[:4]:
                            if t not in kws:
                                kws.append(t)
                kws = list(dict.fromkeys([k for k in kws if 1 <= len(k) <= 20]))[:8]
                if not kws:
                    toks = _re.findall(r"[ê°€-í£]{2,}", " ".join(evs))
                    kws = list(dict.fromkeys(toks))[:5]
                items.append({"emotion": emo, "evidence_sentences": evs, "keywords": kws})
            return items[:4]
        except Exception:
            return []


    # 1ì°¨
    try:
        raw = emo_chain.invoke({"transcript": transcript})
    except Exception:
        raw = ""
    items = _parse_emo_json(raw)
    if not items:
        try:
            raw2 = force_chain.invoke({"transcript": transcript})
        except Exception:
            raw2 = "[]"
        items = _parse_emo_json(raw2)
    if items:
        return items

    # 2ì°¨ í´ë°±(gpt-4o-mini)
    try:
        fb = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, max_tokens=400, api_key=OPENAI_API_KEY)
        fb_chain   = EMO_PROMPT   | fb | parser
        fb_force   = FORCE_PROMPT | fb | parser
        raw = fb_chain.invoke({"transcript": transcript})
        items = _parse_emo_json(raw)
        if not items:
            raw = fb_force.invoke({"transcript": transcript})
            items = _parse_emo_json(raw)
    except Exception:
        items = []
    return items

def build_psych_bullets_from_items(items: List[Dict]) -> str:
    if not items:
        return ""
    lines = []
    for it in items:
        emo = it["emotion"]
        ev  = it["evidence_sentences"][0]
        kws = ", ".join(it["keywords"][:6])
        lines.append(f"- {emo}\n  - ê·¼ê±°ë¬¸ì¥ : â€œ{ev}â€\n  - í‚¤ì›Œë“œ : [{kws}]")
    return "\n".join(lines)

# -------------------------------
# 7) ìš”ì•½ & êµ¬ì¡°í™” ìš”ì•½
# -------------------------------
SYSTEM_PERSONA = '''\
ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ìƒë‹´í•˜ëŠ” ì˜í•™(ì¹˜ë§¤ì§„ë‹¨) ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.
- ì—­í• : í™˜ì/ë³´í˜¸ìì˜ ì„œìˆ (STT í…ìŠ¤íŠ¸)ì„ ì½ê³ , ì§€ì •ëœ í…œí”Œë¦¿ìœ¼ë¡œ ê°„ê²°í•˜ê³  ì²´ê³„ì ì¸ ìš”ì•½ì„ ë§Œë“­ë‹ˆë‹¤.
- íƒœë„: ì •ì¤‘í•˜ê³  ì „ë¬¸ì ì´ë©°, ê³¼ë„í•œ í™•ì‹ /ì§„ë‹¨ì„ í”¼í•˜ê³  ì•ˆì „ìˆ˜ì¹™ì„ ê°•ì¡°í•©ë‹ˆë‹¤.
- ë²”ìœ„: ì¹˜ë§¤, ê²½ë„ì¸ì§€ì¥ì• , ê´€ë ¨ ì¦ìƒ/ê²€ì‚¬/ì¼ìƒ ì•ˆì „/ê°€ì¡± êµìœ¡ì— í•œì •í•©ë‹ˆë‹¤.
- ê¸ˆì§€: ë¬´ê´€í•œ ì •ë³´ëŠ” ìƒì„± ê¸ˆì§€.
- ì¶œë ¥: ë°˜ë“œì‹œ ì•„ë˜ 'ìš”ì•½ í…œí”Œë¦¿'ì˜ <ìš”ì•½> ì„¹ì…˜ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤. (<ì§ˆë¬¸>, <STT>ëŠ” ì¶œë ¥ ê¸ˆì§€)
'''

SUMMARY_TEMPLATE = '''\
<ìš”ì•½>
1. **ì£¼ ì¦ìƒ**
{symptoms_bullets}

2. **ìƒë‹´ë‚´ìš©**
{counselling_bullets}

3. **ì‹¬ë¦¬ìƒíƒœ**
{psych_bullets}

4. **AI í•´ì„**
{ai_interp_bullets}

5. **ì£¼ì˜ì‚¬í•­**
{caution_bullets}
'''

SUMMARISE_PROMPT = ChatPromptTemplate.from_messages(
    [
      ("system", SYSTEM_PERSONA + "\n\n"
        "ì•„ë˜ 'ì°¸ê³  ë¬¸ì„œ(ìš”ì•½)'ëŠ” ì°¸ê³ ìš©ì…ë‹ˆë‹¤. "
        "ê·¼ê±°ê°€ ë¶ˆëª…í™•í•˜ë©´ ë‹¨ì • ëŒ€ì‹  ì¡°ì‹¬ìŠ¤ëŸ¬ìš´ í•´ì„ì„ ì œì‹œí•˜ì„¸ìš”.\n"
        "ë°˜ë“œì‹œ 'ìš”ì•½ í…œí”Œë¦¿' êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©° <ìš”ì•½> ì„¹ì…˜ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤."),
      ("human",
       "ê°€ì´ë“œ ì§ˆë¬¸(ê³ ì • 4ê°œ ì¤‘ ì„ íƒ): {guide_question}\n\n"
       "ì°¸ê³  ë¬¸ì„œ(ìš”ì•½):\n{rag_context}\n\n"
       "ì‚¬ìš©ì STT ì›ë¬¸:\n{transcript}\n\n"
       "ìš”êµ¬ì‚¬í•­:\n"
       "1) 'ì£¼ ì¦ìƒ'ì€ '-' ë¶ˆë¦¿ìœ¼ë¡œ í•µì‹¬ë§Œ.\n"
       "2) 'ìƒë‹´ë‚´ìš©'ì€ êµ¬ì²´ ì‚¬ë¡€ ì¤‘ì‹¬ ë¶ˆë¦¿.\n"
       "3) 'ì‹¬ë¦¬ìƒíƒœ'ëŠ” **ì•„ë˜ ì œê³µëœ ë¶ˆë¦¿ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©**(ë¬¸êµ¬/ìˆœì„œ ë³€ê²½ ê¸ˆì§€). ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤ìŠ¤ë¡œ ì‘ì„±.\n"
       "   ì œê³µ ë¶ˆë¦¿:\n{psych_bullets_fixed}\n"
       "4) 'AI í•´ì„'ì€ ë³‘ëª… ë‹¨ì • ê¸ˆì§€(ì˜ˆ: '~ê°€ëŠ¥ì„± ì‹œì‚¬').\n"
       "5) 'ì£¼ì˜ì‚¬í•­'ì€ ì•ˆì „/ì •ì„œì§€ì§€/ê²€ì§„ ê¶Œê³  í¬í•¨.\n"
       "6) ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥(ì˜¤ì§ <ìš”ì•½> ì„¹ì…˜ë§Œ):\n"
       "{summary_template}\n"
       "ì¶œë ¥ ì‹œ í•œêµ­ì–´ ë”°ì˜´í‘œ(â€œ)ë¥¼ ìœ ì§€í•˜ì„¸ìš”."
      )
    ]
)

ROBUST_SUMMARY_PROMPT = ChatPromptTemplate.from_messages(
    [
      ("system", SYSTEM_PERSONA + "\n\n"
        "ì¶œë ¥ì´ ë¹„ì–´ ìˆê±°ë‚˜ í˜•ì‹ì„ ë”°ë¥´ì§€ ì•Šìœ¼ë©´ ì¦‰ì‹œ N/Aë¡œ ì±„ìš°ê³  í…œí”Œë¦¿ì„ ì™„ì„±í•˜ì„¸ìš”. "
        "ì–´ë– í•œ ê²½ìš°ì—ë„ ë¹ˆ ë¬¸ìì—´ì„ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”."),
      ("human",
       "ê°€ì´ë“œ ì§ˆë¬¸: {guide_question}\n\n"
       "ì°¸ê³  ë¬¸ì„œ(ìš”ì•½):\n{rag_context}\n\n"
       "ì‚¬ìš©ì STT ì›ë¬¸:\n{transcript}\n\n"
       "ì‹¬ë¦¬ìƒíƒœ ì œê³µ ë¶ˆë¦¿(ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©):\n{psych_bullets_fixed}\n\n"
       "ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”:\n{summary_template}")
    ]
)

JSON_SUMMARY_PROMPT = ChatPromptTemplate.from_messages([
    ("system",
     "ì…ë ¥ì— ê¸°ë°˜í•˜ì—¬ ë‹¤ìŒ í•„ë“œë¥¼ ê°–ëŠ” JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ë¼:\n"
     "{"
     "\"primary_symptoms\": [\"...\"], "
     "\"counselling\": [\"...\"], "
     "\"psych\": [{\"emotion\":\"...\",\"evidence\":\"...\",\"keywords\":[\"...\"]}], "
     "\"ai_interpretation\": [\"...\"], "
     "\"cautions\": [\"...\"]"
     "}\n"
     "psychëŠ” ì œê³µëœ psych_itemsë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” STTì™€ ì°¸ê³  ë¬¸ì„œì—ì„œ ìš”ì•½í•˜ë¼."),
    ("human",
     "STT:\n{transcript}\n\n"
     "ì°¸ê³  ë¬¸ì„œ(ìš”ì•½):\n{rag_context}\n\n"
     "psych_items(JSON):\n{psych_items_json}\n\n"
     "ì˜¤ì§ JSON ê°ì²´ë§Œ ì¶œë ¥:")
])

def _make_summary_chain(llm_summary: ChatOpenAI):
    return SUMMARISE_PROMPT | llm_summary | StrOutputParser()

def _make_json_summary_chain(llm_summary: ChatOpenAI):
    return JSON_SUMMARY_PROMPT | llm_summary | StrOutputParser()

# -------------------------------
# 8) íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜
# -------------------------------
def run_summarisation_pipeline(
    transcript: str,
    guide_question_index: int = 3,
    session_id: Optional[str] = None,
    chat_model: Optional[str] = None,
    embed_model: Optional[str] = None,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    models: Optional[Dict[str, str]] = None,
) -> Dict[str, Any]:

    clients = _build_clients(
        chat_model=chat_model,
        embed_model=embed_model,
        temperature=temperature,
        max_tokens=max_tokens,
        models=models,
    )

    # â¶ ì„¸ê·¸ë¨¼íŠ¸ ì •ì±… ì ìš©
    seg_policy = apply_topic_policy(transcript, judge_llm=clients.llm_judge)
    if seg_policy["label"] == "off_topic":
        _update_session(session_id, "off_topic", 0.0)
        return {
            "status": "off_topic",
            "on_topic_prob": 0.0,
            "message": OFF_TOPIC_MESSAGE,
            "summarized_text": OFF_TOPIC_MESSAGE,
            "policy": {
                "coverage": seg_policy["coverage"],
                "dropped_segments": seg_policy["dropped_segments"],
                "offdomain": seg_policy["offdomain"],
                "segments": seg_policy.get("segments", [])
            },
            "used_models": clients.model_ids
        }

    if seg_policy["label"] == "partial":
        working_transcript = seg_policy["filtered_text"] or transcript.strip()
        mix_msg = "ì¹˜ë§¤ì™€ ë¬´ê´€í•œ ìš”ì²­(ì˜ˆ: ì¼ë°˜ ë©”ë‰´ ì¶”ì²œ ë“±)ì€ ì œì™¸í•˜ê³  ìš”ì•½í–ˆìŠµë‹ˆë‹¤."
    else:
        working_transcript = transcript.strip()
        mix_msg = ""

    # â· ë¡œê·¸ìš© prior ì—…ë°ì´íŠ¸
    det = detect_topic(working_transcript, judge_llm=clients.llm_judge, session_id=session_id)
    _update_session(session_id, det["label"], float(det["prob"]))

    # â¸ RAG
    guide_question = GUIDE_QUESTIONS[max(0, min(3, int(guide_question_index)))]
    queries = make_search_queries(working_transcript, clients.llm_query)
    rag_context, rag_sources = build_rag_context_rerank(working_transcript, queries, clients.embeddings)

    # â¹ ê°ì •/ê·¼ê±°/í‚¤ì›Œë“œ
    psych_items = extract_emotions_with_keywords(working_transcript, clients.llm_emo)
    psych_bullets_fixed = build_psych_bullets_from_items(psych_items)

    # âº ìš”ì•½ (ë¹ˆ ì¶œë ¥ ë°©ì§€: ê°•ê±´ í”„ë¡¬í”„íŠ¸ â†’ mini í´ë°±)
    summary_chain = _make_summary_chain(clients.llm_summary)
    try:
        _raw_summary = summary_chain.invoke({
            "transcript": working_transcript,
            "rag_context": rag_context.strip() if rag_context else "(ë¬¸ë§¥ ì—†ìŒ)",
            "guide_question": guide_question,
            "summary_template": SUMMARY_TEMPLATE,
            "psych_bullets_fixed": psych_bullets_fixed if psych_bullets_fixed else "(ì—†ìŒ)",
        })
    except Exception:
        _raw_summary = ""

    def _ensure_summary_not_empty(summary_text: str) -> Tuple[str, str, bool]:
        if summary_text and summary_text.strip():
            return summary_text.strip(), clients.model_ids["summary"], False
        # ê°™ì€ ëª¨ë¸ + ê°•ê±´ í”„ë¡¬í”„íŠ¸
        try:
            robust_chain = ROBUST_SUMMARY_PROMPT | clients.llm_summary | StrOutputParser()
            s2 = robust_chain.invoke({
                "transcript": working_transcript,
                "rag_context": rag_context.strip() if rag_context else "(ë¬¸ë§¥ ì—†ìŒ)",
                "guide_question": guide_question,
                "summary_template": SUMMARY_TEMPLATE,
                "psych_bullets_fixed": psych_bullets_fixed or "(ì—†ìŒ)",
            }).strip()
        except Exception:
            s2 = ""
        if s2:
            return s2, clients.model_ids["summary"], False
        # í´ë°±: gpt-4o-mini
        alt = _build_clients(chat_model="gpt-4o-mini",
                             embed_model=clients.model_ids["embed"],
                             temperature=clients.temperature,
                             max_tokens=clients.max_tokens)
        try:
            alt_chain = ROBUST_SUMMARY_PROMPT | alt.llm_summary | StrOutputParser()
            s3 = alt_chain.invoke({
                "transcript": working_transcript,
                "rag_context": rag_context.strip() if rag_context else "(ë¬¸ë§¥ ì—†ìŒ)",
                "guide_question": guide_question,
                "summary_template": SUMMARY_TEMPLATE,
                "psych_bullets_fixed": psych_bullets_fixed or "(ì—†ìŒ)",
            }).strip()
        except Exception:
            s3 = ""
        return s3, alt.model_ids["summary"], True

    summary_text, summary_model_used, did_fb = _ensure_summary_not_empty(_raw_summary)

    # â» êµ¬ì¡°í™” ìš”ì•½(JSON) â€” BEGIN REPLACE
    def __loose_json_loads(text: str) -> dict:
        """ì½”ë“œíœìŠ¤/ìŠ¤ë§ˆíŠ¸ë”°ì˜´í‘œ/íŠ¸ë ˆì¼ë§ ì½¤ë§ˆê°€ ìˆì–´ë„ ìµœëŒ€í•œ íŒŒì‹±"""
        if not text:
            raise ValueError("empty")
        t = text.strip()
        t = re.sub(r"^```(?:json)?\s*", "", t)
        t = re.sub(r"\s*```$", "", t)
        m = re.search(r"\{.*\}", t, flags=re.DOTALL)
        if not m:
            raise ValueError("no-json-object")
        s = m.group(0)
        s = s.replace("â€œ", "\"").replace("â€", "\"").replace("â€™", "'").replace("â€˜", "'")
        s = re.sub(r",(\s*[}\]])", r"\1", s)
        return json.loads(s)

    def __json_mode_chat(model_id: str):
        """JSON ì „ìš© ì‘ë‹µ ê°•ì œ (langchain-openai ë²„ì „ í˜¸í™˜)"""
        try:
            return ChatOpenAI(
                model=model_id,
                temperature=clients.temperature,
                max_tokens=clients.max_tokens,
                api_key=OPENAI_API_KEY,
                model_kwargs={"response_format": {"type": "json_object"}}
            )
        except TypeError:
            # ì¼ë¶€ ë²„ì „ì€ extra_body ì‚¬ìš©
            return ChatOpenAI(
                model=model_id,
                temperature=clients.temperature,
                max_tokens=clients.max_tokens,
                api_key=OPENAI_API_KEY,
                extra_body={"response_format": {"type": "json_object"}}
            )

    def _make_json_chain_with_model(model_id: str) -> Any:
        return _make_json_summary_chain(__json_mode_chat(model_id))

    def __structured_from_summary_text(summary_text: str) -> Dict[str, Any]:
        """
        <ìš”ì•½> í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ íŒŒì‹±í•´ êµ¬ì¡°í™”. ì‹¬ë¦¬ìƒíƒœëŠ”
        - "ê·¼ê±°ë¬¸ì¥ :" / "í‚¤ì›Œë“œ :" ë¼ì¸ì´ ìˆìœ¼ë©´ ë§¤í•‘, ì—†ìœ¼ë©´ ê°ì •ë§Œ ì±„ì›€.
        """
        data = {
            "primary_symptoms": [],
            "counselling": [],
            "psych": [],
            "ai_interpretation": [],
            "cautions": []
        }
        cur = None
        last_psych = None

        for line in summary_text.splitlines():
            s = line.strip()
            if not s:
                continue

            # ì„¹ì…˜ ì „í™˜
            if "ì£¼ ì¦ìƒ" in s:
                cur = "primary_symptoms"; last_psych = None; continue
            if "ìƒë‹´ë‚´ìš©" in s:
                cur = "counselling"; last_psych = None; continue
            if "ì‹¬ë¦¬ìƒíƒœ" in s:
                cur = "psych"; last_psych = None; continue
            if "AI í•´ì„" in s:
                cur = "ai_interpretation"; last_psych = None; continue
            if "ì£¼ì˜ì‚¬í•­" in s:
                cur = "cautions"; last_psych = None; continue

            # ë¶ˆë¦¿ ì•„ì´í…œ
            if s.startswith("-"):
                item = s[1:].strip()
                if not item or item == "(ì—†ìŒ)":
                    continue
                if cur == "psych":
                    # í•˜ìœ„ ë¼ì¸(ê·¼ê±°ë¬¸ì¥/í‚¤ì›Œë“œ)ì´ ë’¤ë”°ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ê°ì²´ ìƒì„±
                    obj = {"emotion": item, "evidence": "", "keywords": []}
                    data["psych"].append(obj)
                    last_psych = obj
                elif cur:
                    data[cur].append(item)
                continue

            # ì‹¬ë¦¬ìƒíƒœ í•˜ìœ„ ì†ì„±(ê·¼ê±°ë¬¸ì¥/í‚¤ì›Œë“œ)
            if cur == "psych" and last_psych:
                m_ev = re.search(r"ê·¼ê±°ë¬¸ì¥\s*:\s*[\"â€œ]?(.+?)[\"â€]?$", s)
                if m_ev:
                    last_psych["evidence"] = m_ev.group(1).strip()
                    continue
                m_kw = re.search(r"í‚¤ì›Œë“œ\s*:\s*\[(.+)\]$", s)
                if m_kw:
                    toks = [k.strip() for k in m_kw.group(1).split(",")]
                    last_psych["keywords"] = [k for k in toks if k]
                    continue

        return data

    def __ensure_psych_if_empty(structured: Dict[str, Any], text_src: str):
        """ì‹¬ë¦¬ìƒíƒœê°€ ë¹„ì—ˆìœ¼ë©´ ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ìµœì†Œ 1ê°œ ì±„ì›€"""
        if structured.get("psych"):
            return
        emo_map = [
            ("ë¬´ì„­", "ë‘ë ¤ì›€"), ("ë‘ë µ", "ë‘ë ¤ì›€"),
            ("ê±±ì •", "ê±±ì •"), ("ë¶ˆì•ˆ", "ë¶ˆì•ˆê°"),
            ("ì°½í”¼", "ìˆ˜ì¹˜ì‹¬"), ("ë¶€ë„", "ìˆ˜ì¹˜ì‹¬"),
            ("ë‹µë‹µ", "ë‹µë‹µí•¨")
        ]
        added = []
        # ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬
        sents = re.split(r"[.!?\n]+", text_src)
        for kw, emo in emo_map:
            if any(kw in sen for sen in sents):
                ev = ""
                for sen in sents:
                    if kw in sen and 6 <= len(sen.strip()) <= 120:
                        ev = sen.strip()
                        break
                structured.setdefault("psych", []).append(
                    {"emotion": emo, "evidence": ev, "keywords": [kw]}
                )
                added.append(emo)
                break  # ìµœì†Œ 1ê°œë§Œ ë³´ì¥

    # 1ì°¨: ì‹¤ì œ ìš”ì•½ì— ì‚¬ìš©í•œ ëª¨ë¸ë¡œ JSON ìƒì„±(ì—„ê²© JSON ëª¨ë“œ)
    try:
        json_summary_chain = _make_json_chain_with_model(summary_model_used)
        structured_raw = json_summary_chain.invoke({
            "transcript": working_transcript,
            "rag_context": rag_context.strip() if rag_context else "",
            "psych_items_json": json.dumps(psych_items, ensure_ascii=False),
        })
        structured = __loose_json_loads(structured_raw)
    except Exception:
        # 2ì°¨: í´ë°±(gpt-4o-mini)ë¡œ ì¬ì‹œë„
        try:
            json_summary_chain = _make_json_chain_with_model("gpt-4o-mini")
            structured_raw = json_summary_chain.invoke({
                "transcript": working_transcript,
                "rag_context": rag_context.strip() if rag_context else "",
                "psych_items_json": json.dumps(psych_items, ensure_ascii=False),
            })
            structured = __loose_json_loads(structured_raw)
        except Exception:
            # ìµœì¢… í´ë°±: ìš”ì•½ í…ìŠ¤íŠ¸ì—ì„œ ì¬êµ¬ì„±
            structured = __structured_from_summary_text(summary_text)

    # ì‹¬ë¦¬ìƒíƒœ ë¹„ì—ˆìœ¼ë©´ ìµœì†Œ í•œ í•­ëª© ë³´ê°•
    __ensure_psych_if_empty(structured, working_transcript)
    # â» êµ¬ì¡°í™” ìš”ì•½(JSON) â€” END REPLACE


    return {
        "status": "ok",
        "on_topic_prob": seg_policy.get("coverage", 1.0),
        "question_used": guide_question,
        "evidence_spans": [],
        "rag_sources": rag_sources,
        "psych_items": psych_items,
        "summarized_text": summary_text,
        "summary_structured": structured,
        "policy": {
            "label": seg_policy["label"],
            "coverage": seg_policy["coverage"],
            "dropped_segments": seg_policy["dropped_segments"],
            "offdomain": seg_policy["offdomain"],
            "segments": seg_policy.get("segments", []),
            "notice": mix_msg
        },
        "used_models": {
            "summary": summary_model_used,
            "judge":   clients.model_ids["judge"],
            "emotion": clients.model_ids["emotion"],
            "query":   clients.model_ids["query"],
            "embed":   clients.model_ids["embed"],
            "fallback_applied": did_fb
        },
        "temperature": clients.temperature,
        "max_tokens":  clients.max_tokens
    }

def summarise_from_file(
    file_path: str,
    guide_question_index: int = 3,
    session_id: Optional[str] = None,
    **model_overrides
) -> Dict[str, Any]:
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
    with open(file_path, "r", encoding="utf-8") as f:
        transcript = f.read().strip()
    mark_guide_question_shown(session_id)
    return run_summarisation_pipeline(transcript, guide_question_index, session_id, **model_overrides)

# -------------------------------
# ìŒì„± ì±—ë´‡ ì „ìš© ë¶„ì„ í•¨ìˆ˜
# -------------------------------
def analyze_voice_response(
    user_response: str,
    question_context: str = "",
    session_id: Optional[str] = None,
    user_id: str = "",
    chat_model: Optional[str] = None,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> Dict[str, Any]:
    """
    ìŒì„± ì±—ë´‡ ì „ìš© - ì‚¬ìš©ì ë‹µë³€ ë¶„ì„ ë° ìƒë‹´ ì œê³µ
    
    Args:
        user_response: ì‚¬ìš©ìì˜ ìŒì„± ë‹µë³€ í…ìŠ¤íŠ¸
        question_context: ì§ˆë¬¸ ë§¥ë½ (ì˜ˆ: "ìì£¼ ì“°ë˜ ë¬¼ê±´ ì´ë¦„ì´ ê°‘ìê¸° ìƒê°ì•ˆ ë‚œì ì´ ìˆë‚˜ìš”?")
        session_id: ì„¸ì…˜ ID
        user_id: ì‚¬ìš©ì ID
        chat_model: ì‚¬ìš©í•  ì±—ë´‡ ëª¨ë¸
        temperature: ì˜¨ë„ ì„¤ì •
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        
    Returns:
        ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    
    # í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
    clients = _build_clients(
        chat_model=chat_model,
        temperature=temperature,
        max_tokens=max_tokens,
    )
    
    # ì˜¨í† í”½ ê°ì§€
    topic_result = detect_topic(user_response, clients.llm_judge, session_id)
    
    # ì¹˜ë§¤ ê´€ë ¨ì´ ì•„ë‹Œ ê²½ìš°
    if topic_result["label"] == "off_topic":
        return {
            "status": "off_topic",
            "message": OFF_TOPIC_MESSAGE,
            "analysis": {
                "topic_detection": topic_result,
                "user_response": user_response,
                "question_context": question_context
            }
        }
    
    # ì‹¬ë¦¬ìƒíƒœ ë¶„ì„ í”„ë¡¬í”„íŠ¸
    PSYCH_ANALYSIS_PROMPT = ChatPromptTemplate.from_messages([
        ("system", 
         "ë‹¹ì‹ ì€ ì¹˜ë§¤ ê´€ë ¨ ìƒë‹´ì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” ì‹¬ë¦¬ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. "
         "ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ì‹¬ë¦¬ìƒíƒœë¥¼ íŒŒì•…í•˜ê³  ìƒë‹´ì„ ì œê³µí•˜ì„¸ìš”.\n\n"
         "ì¶œë ¥ í˜•ì‹:\n"
         "{{\n"
         "  \"emotional_state\": \"ì£¼ìš” ê°ì • ìƒíƒœ (ì˜ˆ: ê±±ì •, ë‘ë ¤ì›€, ë‹¹í™©, ìˆ˜ì¹˜ì‹¬ ë“±)\",\n"
         "  \"symptom_severity\": \"ì¦ìƒ ì‹¬ê°ë„ (ê²½ë¯¸/ë³´í†µ/ì‹¬ê°)\",\n"
         "  \"key_concerns\": [\"ì£¼ìš” ìš°ë ¤ì‚¬í•­ 1\", \"ì£¼ìš” ìš°ë ¤ì‚¬í•­ 2\"],\n"
         "  \"coping_strategies\": [\"ëŒ€ì²˜ ë°©ë²• 1\", \"ëŒ€ì²˜ ë°©ë²• 2\"],\n"
         "  \"professional_advice\": \"ì „ë¬¸ê°€ ìƒë‹´ ê¶Œì¥ ì—¬ë¶€ (ì˜ˆ: ì¦‰ì‹œ/1ì£¼ì¼ ë‚´/1ê°œì›” ë‚´/ìƒë‹´ ë¶ˆí•„ìš”)\"\n"
         "}}"),
        ("human", 
         f"ì§ˆë¬¸: {question_context}\n"
         f"ì‚¬ìš©ì ë‹µë³€: {user_response}\n\n"
         "ìœ„ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ JSON í˜•íƒœë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.")
    ])
    
    # ìš”ì•½ í”„ë¡¬í”„íŠ¸
    SUMMARY_PROMPT = ChatPromptTemplate.from_messages([
        ("system",
         "ë‹¹ì‹ ì€ ì¹˜ë§¤ ê´€ë ¨ ìƒë‹´ì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” ì˜ë£Œì§„ì…ë‹ˆë‹¤. "
         "ì‚¬ìš©ìì˜ ë‹µë³€ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ê³  í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì„¸ìš”.\n\n"
         "ì¶œë ¥ í˜•ì‹:\n"
         "{{\n"
         "  \"main_points\": [\"í•µì‹¬ ë‚´ìš© 1\", \"í•µì‹¬ ë‚´ìš© 2\"],\n"
         "  \"symptom_description\": \"ì¦ìƒì— ëŒ€í•œ êµ¬ì²´ì  ì„¤ëª…\",\n"
         "  \"impact_on_daily_life\": \"ì¼ìƒìƒí™œì— ë¯¸ì¹˜ëŠ” ì˜í–¥\",\n"
         "  \"frequency\": \"ì¦ìƒ ë°œìƒ ë¹ˆë„ (ì˜ˆ: ê°€ë”/ìì£¼/ë§¤ì¼)\"\n"
         "}}"),
        ("human",
         f"ì§ˆë¬¸: {question_context}\n"
         f"ì‚¬ìš©ì ë‹µë³€: {user_response}\n\n"
         "ìœ„ ë‹µë³€ì„ ìš”ì•½í•˜ì—¬ JSON í˜•íƒœë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.")
    ])
    
    # ì£¼ì˜ì‚¬í•­ í”„ë¡¬í”„íŠ¸
    CAUTION_PROMPT = ChatPromptTemplate.from_messages([
        ("system",
         "ë‹¹ì‹ ì€ ì¹˜ë§¤ ê´€ë ¨ ìƒë‹´ì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” ì˜ë£Œì§„ì…ë‹ˆë‹¤. "
         "ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ì£¼ì˜ì‚¬í•­ê³¼ ê¶Œì¥ì‚¬í•­ì„ ì œê³µí•˜ì„¸ìš”.\n\n"
         "ì¶œë ¥ í˜•ì‹:\n"
         "{{\n"
         "  \"immediate_actions\": [\"ì¦‰ì‹œ ì·¨í•´ì•¼ í•  í–‰ë™ 1\", \"ì¦‰ì‹œ ì·¨í•´ì•¼ í•  í–‰ë™ 2\"],\n"
         "  \"safety_measures\": [\"ì•ˆì „ ì¡°ì¹˜ 1\", \"ì•ˆì „ ì¡°ì¹˜ 2\"],\n"
         "  \"monitoring_points\": [\"ê´€ì°°í•´ì•¼ í•  ì  1\", \"ê´€ì°°í•´ì•¼ í•  ì  2\"],\n"
         "  \"when_to_seek_help\": \"ì–¸ì œ ì „ë¬¸ê°€ ë„ì›€ì„ ë°›ì•„ì•¼ í•˜ëŠ”ì§€\",\n"
         "  \"family_guidance\": \"ê°€ì¡±ì´ ì·¨í•´ì•¼ í•  ì¡°ì¹˜\"\n"
         "}}"),
        ("human",
         f"ì§ˆë¬¸: {question_context}\n"
         f"ì‚¬ìš©ì ë‹µë³€: {user_response}\n\n"
         "ìœ„ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ì£¼ì˜ì‚¬í•­ì„ JSON í˜•íƒœë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.")
    ])
    
    try:
        # ê° ë¶„ì„ ì‹¤í–‰
        psych_chain = PSYCH_ANALYSIS_PROMPT | clients.llm_emo | StrOutputParser()
        summary_chain = SUMMARY_PROMPT | clients.llm_summary | StrOutputParser()
        caution_chain = CAUTION_PROMPT | clients.llm_summary | StrOutputParser()
        
        # ë³‘ë ¬ë¡œ ë¶„ì„ ì‹¤í–‰ (ì‹¤ì œë¡œëŠ” ìˆœì°¨ ì‹¤í–‰)
        psych_result = psych_chain.invoke({})
        summary_result = summary_chain.invoke({})
        caution_result = caution_chain.invoke({})
        
        # JSON íŒŒì‹±
        try:
            psych_data = _json_loose_loads(psych_result)
        except:
            psych_data = {"error": "ì‹¬ë¦¬ìƒíƒœ ë¶„ì„ íŒŒì‹± ì‹¤íŒ¨"}
            
        try:
            summary_data = _json_loose_loads(summary_result)
        except:
            summary_data = {"error": "ìš”ì•½ ë¶„ì„ íŒŒì‹± ì‹¤íŒ¨"}
            
        try:
            caution_data = _json_loose_loads(caution_result)
        except:
            caution_data = {"error": "ì£¼ì˜ì‚¬í•­ ë¶„ì„ íŒŒì‹± ì‹¤íŒ¨"}
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "status": "success",
            "session_id": session_id,
            "user_id": user_id,
            "timestamp": time.time(),
            "question_context": question_context,
            "user_response": user_response,
            "topic_detection": topic_result,
            "analysis": {
                "summary": summary_data,
                "psychological_state": psych_data,
                "cautions": caution_data
            },
            "model_info": {
                "chat_model": clients.model_ids["summary"],
                "temperature": clients.temperature,
                "max_tokens": clients.max_tokens
            }
        }
        
        return result
        
    except Exception as e:
        print(f"ìŒì„± ì‘ë‹µ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {
            "status": "error",
            "error": str(e),
            "session_id": session_id,
            "user_id": user_id,
            "timestamp": time.time()
        }
