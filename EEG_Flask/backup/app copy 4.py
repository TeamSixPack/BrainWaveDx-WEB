# ---------------------------------------------------------
# Flask ë”¥ëŸ¬ë‹ ì¶”ë¡  ì„œë²„ (Spring ì—°ë™ìš©)
# - /infer : JSON ì…ë ¥ ë°›ì•„ ë™ê¸°/ë¹„ë™ê¸° ì¶”ë¡  í›„ ê²°ê³¼ ë°˜í™˜/ì½œë°±
# - ë””ë°”ì´ìŠ¤ë³„ ì±„ë„ ì„¸íŠ¸ ê°•ì œ ì£¼ì… + subject-level í™•ë¥ /íŒì • í¬í•¨
# ---------------------------------------------------------
import os
import uuid
import traceback
from concurrent.futures import ThreadPoolExecutor

import requests
from flask import Flask, request, jsonify
from flask_cors import CORS

import torch
from eeg_model import (
    load_hf_model,
    predict_from_eeglab_file,
    extract_subject_id,
)

# -----------------------------
# ê¸°ë³¸ ë””ë°”ì´ìŠ¤(í™˜ê²½ë³€ìˆ˜ë¡œ ë®ì–´ì“°ê¸° ê°€ëŠ¥)
# -----------------------------
DEVICE_NAME = 'muse'
# DEVICE_NAME = 'hybrid_black'
# DEVICE_NAME = 'union10'

EEG_DEVICE = os.getenv("EEG_DEVICE", DEVICE_NAME).strip()
HF_USERNAME = os.getenv("HF_USERNAME", "ardor924").strip()
MODEL_VER   = os.getenv("MODEL_VER", "Ver14").strip()

DEVICE_TO_CHANNELS = {
    "hybrid_black": 8,
    "muse": 4,
    "union10": 10,
}

# ë””ë°”ì´ìŠ¤ë³„ ì±„ë„ ì´ë¦„(ìˆœì„œ ê³ ì •)
DEVICE_TO_PICK_CHANNELS = {
    "union10":      ['T5', 'T6', 'F7', 'F8', 'Fz', 'C3', 'Cz', 'C4', 'Pz', 'O1'],
    "muse":         ['T5', 'T6', 'F7', 'F8'],
    "hybrid_black": ['Fz', 'C3', 'Cz', 'C4', 'Pz', 'T5', 'T6', 'O1'],
}
def get_pick_channels(device_name: str):
    return DEVICE_TO_PICK_CHANNELS.get(device_name, DEVICE_TO_PICK_CHANNELS[DEVICE_NAME])

CHANNEL_LEN = str(DEVICE_TO_CHANNELS.get(EEG_DEVICE, 8))
DEFAULT_REPO = f"{HF_USERNAME}/EEGNetV4-{CHANNEL_LEN}ch-{EEG_DEVICE}-{MODEL_VER}"
HF_REPO_ID = os.getenv("HF_REPO_ID", DEFAULT_REPO).strip()

HF_TOKEN   = os.getenv("HF_TOKEN", None)     # ex) hf_xxx...
DEVICE_OPT = os.getenv("DEVICE", None)       # 'cuda' | 'cpu' | None
THREADS    = int(os.getenv("WORKERS", "2"))
DATA_ROOT  = os.getenv("DATA_ROOT", ".")
ENFORCE_2MIN_DEFAULT = os.getenv("ENFORCE_2MIN", "true").lower() == "true"

SPRING_URL       = os.getenv("SPRING_URL", "http://localhost:8090/eeg/result").strip()
SPRING_AUTO_POST = os.getenv("SPRING_AUTO_POST", "true").lower() == "true"

EXEC = ThreadPoolExecutor(max_workers=THREADS)
JOBS = {}

def _boollike(x, default=False):
    if isinstance(x, bool):
        return x
    if x is None:
        return default
    s = str(x).strip().lower()
    if s in ("1", "true", "yes", "y", "on"):
        return True
    if s in ("0", "false", "no", "n", "off"):
        return False
    return default

def create_app():
    app = Flask(__name__)
    CORS(app)

    # -----------------------------
    # ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ)
    # -----------------------------
    global MODEL, CFG, DEVICE_RESOLVED
    try:
        MODEL, CFG = load_hf_model(HF_REPO_ID, device=DEVICE_OPT, token=HF_TOKEN)
    except Exception as e:
        raise SystemExit(
            f"\nâŒ Hugging Face ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨\n"
            f"   - repo_id: {HF_REPO_ID}\n"
            f"   - reason : {e}\n"
            f"   - í™•ì¸ì‚¬í•­: 1) repo_id ì •í™•ì„±  2) ë¹„ê³µê°œ ë ˆí¬ë©´ HF_TOKEN  3) EEG_DEVICE/HF_USERNAME/MODEL_VER ì¼ì¹˜\n"
        )

    DEVICE_RESOLVED = "cuda" if torch.cuda.is_available() else "cpu"
    app.logger.info(
        f"âœ… HF model loaded: {HF_REPO_ID} | device={DEVICE_RESOLVED} | n_chans={CFG.get('n_chans')} | labels={CFG.get('idx_to_label')}"
    )
    app.logger.info(f"âœ… server EEG_DEVICE={EEG_DEVICE}, server pick_channels={get_pick_channels(EEG_DEVICE)}")

    expected_ch = DEVICE_TO_CHANNELS.get(EEG_DEVICE)
    cfg_ch = int(CFG.get("n_chans", expected_ch))
    if expected_ch != cfg_ch:
        app.logger.warning(f"âš  ëª¨ë¸ n_chans({cfg_ch}) vs ì„œë²„ ë””ë°”ì´ìŠ¤({EEG_DEVICE}:{expected_ch}) ë¶ˆì¼ì¹˜ ê°€ëŠ¥ â€” HF_REPO_ID í™•ì¸ í•„ìš”.")

    @app.route("/health", methods=["GET"])
    def health():
        return jsonify({
            "status": "ok",
            "device": DEVICE_RESOLVED,
            "repo": HF_REPO_ID,
            "spring_auto_post": SPRING_AUTO_POST,
            "spring_url": SPRING_URL,
            "eeg_device": EEG_DEVICE,
            "channels": CFG.get("n_chans"),
            "server_pick_channels": get_pick_channels(EEG_DEVICE),
        }), 200

    @app.route("/infer", methods=["POST"])
    def infer():
        try:
            payload = request.get_json(force=True)
            if payload is None:
                return jsonify({"error": "no JSON payload"}), 400

            # ğŸ” ë“¤ì–´ì˜¨ ê°’ ë¡œê¹…(í•„ìš”í•˜ë©´ ì£¼ì„ ì²˜ë¦¬)
            app.logger.info(f"[infer] payload keys={list(payload.keys())}")


            # --- ì…ë ¥ í‚¤: camelCase/snake_case ëª¨ë‘ í—ˆìš© ---
            file_path = (payload.get("file_path")
                         or payload.get("filePath") or "").strip()
            if not file_path:
                return jsonify({"error": "file_path is required (.set)"}), 400
            if not os.path.isabs(file_path):
                file_path = os.path.normpath(os.path.join(DATA_ROOT, file_path))
            if not os.path.exists(file_path):
                return jsonify({"error": f"file not found: {file_path}"}), 404

            req_device = (payload.get("device")
                          or payload.get("channel_name")
                          or payload.get("channelName")
                          or EEG_DEVICE).strip()
            if req_device != EEG_DEVICE:
                return jsonify({
                    "error": "device mismatch",
                    "message": f"Server loaded for '{EEG_DEVICE}'. You requested '{req_device}'. "
                               f"Restart with EEG_DEVICE={req_device} (and matching HF_REPO_ID)."
                }), 400

            pick_channels = get_pick_channels(EEG_DEVICE)
            cfg_for_run = dict(CFG)
            cfg_for_run["pick_channels"] = pick_channels
            cfg_for_run["n_chans"] = len(pick_channels)

            # subject_id: ê²½ë¡œì—ì„œ ì¶”ì¶œì´ ì§„ì‹¤
            sid_from_path = extract_subject_id(file_path)
            req_sid = payload.get("subject_id") or payload.get("subjectId")
            if req_sid and req_sid != sid_from_path:
                app.logger.warning(f"subject_id mismatch ignored: req={req_sid} path={sid_from_path}")
            subject_id = sid_from_path

            # true label & 2ë¶„ ì˜µì…˜ (ë‘˜ ë‹¤ ì–‘ì‹ í—ˆìš©)
            true_label   = payload.get("true_label") or payload.get("trueLabel")
            enforce_2min = payload.get("enforce_two_minutes")
            if enforce_2min is None:
                enforce_2min = payload.get("enforceTwoMinutes", ENFORCE_2MIN_DEFAULT)
            enforce_2min = _boollike(enforce_2min, ENFORCE_2MIN_DEFAULT)

            callback_url = payload.get("callback_url") or payload.get("callbackUrl")

            # ë¹„ë™ê¸°
            if callback_url:
                job_id = str(uuid.uuid4())
                JOBS[job_id] = {"status": "queued", "result": None}
                EXEC.submit(_run_infer_and_callback, job_id, file_path, subject_id,
                            true_label, callback_url, enforce_2min, cfg_for_run)
                return jsonify({"status": "accepted", "job_id": job_id, "file_path": file_path}), 202

            # ë™ê¸°
            out = predict_from_eeglab_file(
                file_path=file_path, model=MODEL, cfg=cfg_for_run,
                device=DEVICE_RESOLVED, true_label=true_label,
                enforce_two_minutes=enforce_2min
            )

            def camelize(d: dict) -> dict:
                def ckey(k: str) -> str:
                    parts = k.split('_')
                    return parts[0] + ''.join(p.title() for p in parts[1:])
                return {ckey(k): v for k, v in d.items()}

            res = {
                "subject_id": out["subject_id"],
                "file_path": out["file_path"],
                "n_segments": out["n_segments"],
                "segment_counts": out["segment_counts"],
                "segment_majority_index": out["segment_majority_index"],
                "segment_majority_label": out["segment_majority_label"],
                "segment_accuracy": out["segment_accuracy"],
                "prob_mean": out["prob_mean"],
                "subject_probs": out["subject_probs"],
                "subject_pred_index": out["subject_pred_index"],
                "subject_pred_label": out["subject_pred_label"],
                "subject_accuracy": out["subject_accuracy"],
                "true_label": out.get("true_label"),
                "true_label_raw": out.get("true_label_raw"),  # ğŸ” ë””ë²„ê¹…ìš©
                "channels_used": out["channels_used"],
                "channels_map": out["channels_map"],
                "raw_channel_names": out["raw_channel_names"],
                "window": out["window"],
            }
            res.update(camelize(res))

            if SPRING_AUTO_POST:
                EXEC.submit(_post_to_spring, SPRING_URL, {"result": res})

            return jsonify({"status": "ok", "result": res}), 200

        except ValueError as ve:
            return jsonify({"error": str(ve)}), 400
        except Exception as e:
            traceback.print_exc()
            return jsonify({"error": str(e)}), 500

    @app.route("/result/<job_id>", methods=["GET"])
    def result(job_id):
        info = JOBS.get(job_id)
        if not info:
            return jsonify({"error": "invalid job_id"}), 404
        return jsonify(info), 200

    return app

# =========================
# ë‚´ë¶€ ìœ í‹¸
# =========================
def _post_to_spring(url: str, payload: dict):
    try:
        resp = requests.post(url, json=payload, timeout=15)
        resp.raise_for_status()
    except Exception as e:
        print(f"[WARN] Spring POST failed â†’ {url} | reason={e}")

def _run_infer(file_path: str, subject_id: str, true_label, enforce_2min: bool, cfg_for_run: dict):
    out = predict_from_eeglab_file(
        file_path=file_path,
        model=MODEL,
        cfg=cfg_for_run,  # â† ì´ ìš”ì²­ì— ë§ì¶˜ pick_channels ê°•ì œ ì‚¬ìš©
        device="cuda" if torch.cuda.is_available() else "cpu",
        true_label=true_label,
        enforce_two_minutes=enforce_2min
    )
    # snake+camel ëª¨ë‘ ê°€ì§„ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ë ¤ë©´ app.infer()ì—ì„œ ë§Œë“  í¬ë§·ì„ ì¬ì‚¬ìš©í•˜ëŠ” í¸ì´ ë‚«ì§€ë§Œ
    # ì—¬ê¸°ì„œëŠ” í…ŒìŠ¤íŠ¸/ì½œë°± ìš©ë„ë¡œ ìµœì†Œ í•„ë“œë§Œ ìœ ì§€í•©ë‹ˆë‹¤.
    res = {
        "subject_id": out["subject_id"],
        "file_path": out["file_path"],
        "n_segments": out["n_segments"],
        "segment_counts": out["segment_counts"],
        "segment_majority_index": out["segment_majority_index"],
        "segment_majority_label": out["segment_majority_label"],
        "segment_accuracy": out["segment_accuracy"],
        "prob_mean": out["prob_mean"],
        "subject_probs": out["subject_probs"],
        "subject_pred_index": out["subject_pred_index"],
        "subject_pred_label": out["subject_pred_label"],
        "subject_accuracy": out["subject_accuracy"],
        "true_label": out.get("true_label"),
        "channels_used": out["channels_used"],
        "channels_map": out["channels_map"],
        "raw_channel_names": out["raw_channel_names"],
        "window": out["window"],
    }
    return res


def _run_infer_and_callback(job_id: str, file_path: str, subject_id: str, true_label, callback_url: str, enforce_2min: bool, cfg_for_run: dict):
    JOBS[job_id]["status"] = "running"
    try:
        result = _run_infer(file_path, subject_id, true_label, enforce_2min, cfg_for_run)
        JOBS[job_id]["result"] = result
        JOBS[job_id]["status"] = "done"

        try:
            resp = requests.post(callback_url, json={"job_id": job_id, "result": result}, timeout=15)
            resp.raise_for_status()
        except Exception as e:
            print(f"[WARN] user callback POST failed â†’ {callback_url} | reason={e}")

        if SPRING_AUTO_POST and (callback_url.strip() != SPRING_URL.strip()):
            _post_to_spring(SPRING_URL, {"job_id": job_id, "result": result})

    except Exception as e:
        JOBS[job_id]["status"] = "error"
        JOBS[job_id]["result"] = {"error": str(e)}

# =========================
# ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
# =========================
if __name__ == "__main__":
    app = create_app()
    app.run(host="127.0.0.1", port=8000)
