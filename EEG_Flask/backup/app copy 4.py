# ---------------------------------------------------------
# Flask Îî•Îü¨Îãù Ï∂îÎ°† ÏÑúÎ≤Ñ (Spring Ïó∞ÎèôÏö©)
# - /infer : JSON ÏûÖÎ†• Î∞õÏïÑ ÎèôÍ∏∞/ÎπÑÎèôÍ∏∞ Ï∂îÎ°† ÌõÑ Í≤∞Í≥º Î∞òÌôò/ÏΩúÎ∞±
# - ÎîîÎ∞îÏù¥Ïä§Î≥Ñ Ï±ÑÎÑê ÏÑ∏Ìä∏ Í∞ïÏ†ú Ï£ºÏûÖ + subject-level ÌôïÎ•†/ÌåêÏ†ï Ìè¨Ìï®
# ---------------------------------------------------------
import os
import uuid
import traceback
from concurrent.futures import ThreadPoolExecutor

import requests
from flask import Flask, request, jsonify
from flask_cors import CORS

import torch
from eeg_model import (
    load_hf_model,
    predict_from_eeglab_file,
    extract_subject_id,
)

# -----------------------------
# Í∏∞Î≥∏ ÎîîÎ∞îÏù¥Ïä§(ÌôòÍ≤ΩÎ≥ÄÏàòÎ°ú ÎçÆÏñ¥Ïì∞Í∏∞ Í∞ÄÎä•)
# -----------------------------
DEVICE_NAME = 'muse'
# DEVICE_NAME = 'hybrid_black'
# DEVICE_NAME = 'union10'

EEG_DEVICE = os.getenv("EEG_DEVICE", DEVICE_NAME).strip()
HF_USERNAME = os.getenv("HF_USERNAME", "ardor924").strip()
MODEL_VER   = os.getenv("MODEL_VER", "Ver14").strip()

DEVICE_TO_CHANNELS = {
    "hybrid_black": 8,
    "muse": 4,
    "union10": 10,
}

# ÎîîÎ∞îÏù¥Ïä§Î≥Ñ Ï±ÑÎÑê Ïù¥Î¶Ñ(ÏàúÏÑú Í≥†Ï†ï)
DEVICE_TO_PICK_CHANNELS = {
    "union10":      ['T5', 'T6', 'F7', 'F8', 'Fz', 'C3', 'Cz', 'C4', 'Pz', 'O1'],
    "muse":         ['T5', 'T6', 'F7', 'F8'],
    "hybrid_black": ['Fz', 'C3', 'Cz', 'C4', 'Pz', 'T5', 'T6', 'O1'],
}
def get_pick_channels(device_name: str):
    return DEVICE_TO_PICK_CHANNELS.get(device_name, DEVICE_TO_PICK_CHANNELS[DEVICE_NAME])

CHANNEL_LEN = str(DEVICE_TO_CHANNELS.get(EEG_DEVICE, 8))
DEFAULT_REPO = f"{HF_USERNAME}/EEGNetV4-{CHANNEL_LEN}ch-{EEG_DEVICE}-{MODEL_VER}"
HF_REPO_ID = os.getenv("HF_REPO_ID", DEFAULT_REPO).strip()

HF_TOKEN   = os.getenv("HF_TOKEN", None)     # ex) hf_xxx...
DEVICE_OPT = os.getenv("DEVICE", None)       # 'cuda' | 'cpu' | None
THREADS    = int(os.getenv("WORKERS", "2"))
DATA_ROOT  = os.getenv("DATA_ROOT", ".")
ENFORCE_2MIN_DEFAULT = os.getenv("ENFORCE_2MIN", "true").lower() == "true"

SPRING_URL       = os.getenv("SPRING_URL", "http://localhost:8090/eeg/result").strip()
SPRING_AUTO_POST = os.getenv("SPRING_AUTO_POST", "true").lower() == "true"

EXEC = ThreadPoolExecutor(max_workers=THREADS)
JOBS = {}

def _boollike(x, default=False):
    if isinstance(x, bool):
        return x
    if x is None:
        return default
    s = str(x).strip().lower()
    if s in ("1", "true", "yes", "y", "on"):
        return True
    if s in ("0", "false", "no", "n", "off"):
        return False
    return default

def create_app():
    app = Flask(__name__)
    CORS(app)

    # -----------------------------
    # Î™®Îç∏ Î°úÎìú (ÏÑúÎ≤Ñ ÏãúÏûë Ïãú 1Ìöå)
    # -----------------------------
    global MODEL, CFG, DEVICE_RESOLVED
    try:
        MODEL, CFG = load_hf_model(HF_REPO_ID, device=DEVICE_OPT, token=HF_TOKEN)
    except Exception as e:
        raise SystemExit(
            f"\n‚ùå Hugging Face Î™®Îç∏ Î°úÎìú Ïã§Ìå®\n"
            f"   - repo_id: {HF_REPO_ID}\n"
            f"   - reason : {e}\n"
            f"   - ÌôïÏù∏ÏÇ¨Ìï≠: 1) repo_id Ï†ïÌôïÏÑ±  2) ÎπÑÍ≥µÍ∞ú Î†àÌè¨Î©¥ HF_TOKEN  3) EEG_DEVICE/HF_USERNAME/MODEL_VER ÏùºÏπò\n"
        )

    DEVICE_RESOLVED = "cuda" if torch.cuda.is_available() else "cpu"
    app.logger.info(
        f"‚úÖ HF model loaded: {HF_REPO_ID} | device={DEVICE_RESOLVED} | n_chans={CFG.get('n_chans')} | labels={CFG.get('idx_to_label')}"
    )
    app.logger.info(f"‚úÖ server EEG_DEVICE={EEG_DEVICE}, server pick_channels={get_pick_channels(EEG_DEVICE)}")

    expected_ch = DEVICE_TO_CHANNELS.get(EEG_DEVICE)
    cfg_ch = int(CFG.get("n_chans", expected_ch))
    if expected_ch != cfg_ch:
        app.logger.warning(f"‚ö† Î™®Îç∏ n_chans({cfg_ch}) vs ÏÑúÎ≤Ñ ÎîîÎ∞îÏù¥Ïä§({EEG_DEVICE}:{expected_ch}) Î∂àÏùºÏπò Í∞ÄÎä• ‚Äî HF_REPO_ID ÌôïÏù∏ ÌïÑÏöî.")

    @app.route("/health", methods=["GET"])
    def health():
        return jsonify({
            "status": "ok",
            "device": DEVICE_RESOLVED,
            "repo": HF_REPO_ID,
            "spring_auto_post": SPRING_AUTO_POST,
            "spring_url": SPRING_URL,
            "eeg_device": EEG_DEVICE,
            "channels": CFG.get("n_chans"),
            "server_pick_channels": get_pick_channels(EEG_DEVICE),
        }), 200

    @app.route("/infer", methods=["POST"])
    def infer():
        try:
            payload = request.get_json(force=True)
            if payload is None:
                return jsonify({"error": "no JSON payload"}), 400

            # üîé Îì§Ïñ¥Ïò® Í∞í Î°úÍπÖ(ÌïÑÏöîÌïòÎ©¥ Ï£ºÏÑù Ï≤òÎ¶¨)
            app.logger.info(f"[infer] payload keys={list(payload.keys())}")


            # --- ÏûÖÎ†• ÌÇ§: camelCase/snake_case Î™®Îëê ÌóàÏö© ---
            file_path = (payload.get("file_path")
                         or payload.get("filePath") or "").strip()
            if not file_path:
                return jsonify({"error": "file_path is required (.set)"}), 400
            if not os.path.isabs(file_path):
                file_path = os.path.normpath(os.path.join(DATA_ROOT, file_path))
            if not os.path.exists(file_path):
                return jsonify({"error": f"file not found: {file_path}"}), 404

            req_device = (payload.get("device")
                          or payload.get("channel_name")
                          or payload.get("channelName")
                          or EEG_DEVICE).strip()
            if req_device != EEG_DEVICE:
                return jsonify({
                    "error": "device mismatch",
                    "message": f"Server loaded for '{EEG_DEVICE}'. You requested '{req_device}'. "
                               f"Restart with EEG_DEVICE={req_device} (and matching HF_REPO_ID)."
                }), 400

            pick_channels = get_pick_channels(EEG_DEVICE)
            cfg_for_run = dict(CFG)
            cfg_for_run["pick_channels"] = pick_channels
            cfg_for_run["n_chans"] = len(pick_channels)

            # subject_id: Í≤ΩÎ°úÏóêÏÑú Ï∂îÏ∂úÏù¥ ÏßÑÏã§
            sid_from_path = extract_subject_id(file_path)
            req_sid = payload.get("subject_id") or payload.get("subjectId")
            if req_sid and req_sid != sid_from_path:
                app.logger.warning(f"subject_id mismatch ignored: req={req_sid} path={sid_from_path}")
            subject_id = sid_from_path

            # true label & 2Î∂Ñ ÏòµÏÖò (Îëò Îã§ ÏñëÏãù ÌóàÏö©)
            true_label   = payload.get("true_label") or payload.get("trueLabel")
            enforce_2min = payload.get("enforce_two_minutes")
            if enforce_2min is None:
                enforce_2min = payload.get("enforceTwoMinutes", ENFORCE_2MIN_DEFAULT)
            enforce_2min = _boollike(enforce_2min, ENFORCE_2MIN_DEFAULT)

            callback_url = payload.get("callback_url") or payload.get("callbackUrl")

            # ÎπÑÎèôÍ∏∞
            if callback_url:
                job_id = str(uuid.uuid4())
                JOBS[job_id] = {"status": "queued", "result": None}
                EXEC.submit(_run_infer_and_callback, job_id, file_path, subject_id,
                            true_label, callback_url, enforce_2min, cfg_for_run)
                return jsonify({"status": "accepted", "job_id": job_id, "file_path": file_path}), 202

            # ÎèôÍ∏∞
            out = predict_from_eeglab_file(
                file_path=file_path, model=MODEL, cfg=cfg_for_run,
                device=DEVICE_RESOLVED, true_label=true_label,
                enforce_two_minutes=enforce_2min
            )

            def camelize(d: dict) -> dict:
                def ckey(k: str) -> str:
                    parts = k.split('_')
                    return parts[0] + ''.join(p.title() for p in parts[1:])
                return {ckey(k): v for k, v in d.items()}

            res = {
                "subject_id": out["subject_id"],
                "file_path": out["file_path"],
                "n_segments": out["n_segments"],
                "segment_counts": out["segment_counts"],
                "segment_majority_index": out["segment_majority_index"],
                "segment_majority_label": out["segment_majority_label"],
                "segment_accuracy": out["segment_accuracy"],
                "prob_mean": out["prob_mean"],
                "subject_probs": out["subject_probs"],
                "subject_pred_index": out["subject_pred_index"],
                "subject_pred_label": out["subject_pred_label"],
                "subject_accuracy": out["subject_accuracy"],
                "true_label": out.get("true_label"),
                "true_label_raw": out.get("true_label_raw"),  # üîé ÎîîÎ≤ÑÍπÖÏö©
                "channels_used": out["channels_used"],
                "channels_map": out["channels_map"],
                "raw_channel_names": out["raw_channel_names"],
                "window": out["window"],
            }
            res.update(camelize(res))

            if SPRING_AUTO_POST:
                EXEC.submit(_post_to_spring, SPRING_URL, {"result": res})

            return jsonify({"status": "ok", "result": res}), 200

        except ValueError as ve:
            return jsonify({"error": str(ve)}), 400
        except Exception as e:
            traceback.print_exc()
            return jsonify({"error": str(e)}), 500

    @app.route("/result/<job_id>", methods=["GET"])
    def result(job_id):
        info = JOBS.get(job_id)
        if not info:
            return jsonify({"error": "invalid job_id"}), 404
        return jsonify(info), 200

    return app

# =========================
# ÎÇ¥Î∂Ä Ïú†Ìã∏
# =========================
def _post_to_spring(url: str, payload: dict):
    try:
        resp = requests.post(url, json=payload, timeout=15)
        resp.raise_for_status()
    except Exception as e:
        print(f"[WARN] Spring POST failed ‚Üí {url} | reason={e}")

def _run_infer(file_path: str, subject_id: str, true_label, enforce_2min: bool, cfg_for_run: dict):
    out = predict_from_eeglab_file(
        file_path=file_path,
        model=MODEL,
        cfg=cfg_for_run,  # ‚Üê Ïù¥ ÏöîÏ≤≠Ïóê ÎßûÏ∂ò pick_channels Í∞ïÏ†ú ÏÇ¨Ïö©
        device="cuda" if torch.cuda.is_available() else "cpu",
        true_label=true_label,
        enforce_two_minutes=enforce_2min
    )
    # snake+camel Î™®Îëê Í∞ÄÏßÑ Í≤∞Í≥ºÎ•º Í∑∏ÎåÄÎ°ú Î∞òÌôòÌïòÎ†§Î©¥ app.infer()ÏóêÏÑú ÎßåÎì† Ìè¨Îß∑ÏùÑ Ïû¨ÏÇ¨Ïö©ÌïòÎäî Ìé∏Ïù¥ ÎÇ´ÏßÄÎßå
    # Ïó¨Í∏∞ÏÑúÎäî ÌÖåÏä§Ìä∏/ÏΩúÎ∞± Ïö©ÎèÑÎ°ú ÏµúÏÜå ÌïÑÎìúÎßå Ïú†ÏßÄÌï©ÎãàÎã§.
    res = {
        "subject_id": out["subject_id"],
        "file_path": out["file_path"],
        "n_segments": out["n_segments"],
        "segment_counts": out["segment_counts"],
        "segment_majority_index": out["segment_majority_index"],
        "segment_majority_label": out["segment_majority_label"],
        "segment_accuracy": out["segment_accuracy"],
        "prob_mean": out["prob_mean"],
        "subject_probs": out["subject_probs"],
        "subject_pred_index": out["subject_pred_index"],
        "subject_pred_label": out["subject_pred_label"],
        "subject_accuracy": out["subject_accuracy"],
        "true_label": out.get("true_label"),
        "channels_used": out["channels_used"],
        "channels_map": out["channels_map"],
        "raw_channel_names": out["raw_channel_names"],
        "window": out["window"],
    }
    return res


def _run_infer_and_callback(job_id: str, file_path: str, subject_id: str, true_label, callback_url: str, enforce_2min: bool, cfg_for_run: dict):
    JOBS[job_id]["status"] = "running"
    try:
        result = _run_infer(file_path, subject_id, true_label, enforce_2min, cfg_for_run)
        JOBS[job_id]["result"] = result
        JOBS[job_id]["status"] = "done"

        try:
            resp = requests.post(callback_url, json={"job_id": job_id, "result": result}, timeout=15)
            resp.raise_for_status()
        except Exception as e:
            print(f"[WARN] user callback POST failed ‚Üí {callback_url} | reason={e}")

        if SPRING_AUTO_POST and (callback_url.strip() != SPRING_URL.strip()):
            _post_to_spring(SPRING_URL, {"job_id": job_id, "result": result})

    except Exception as e:
        JOBS[job_id]["status"] = "error"
        JOBS[job_id]["result"] = {"error": str(e)}

# =========================
# ÏóîÌä∏Î¶¨ Ìè¨Ïù∏Ìä∏
# =========================
if __name__ == "__main__":
    app = create_app()
    app.run(host="127.0.0.1", port=8000)
